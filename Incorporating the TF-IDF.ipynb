{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towards new features\n",
    "\n",
    "Unfortunately, parameter tuning for both Random Forests and SVM seems not to lead to large increases in classification performance. We might be able to increase, however, by extracting new features. Currently, the similarity score is calculated as in the paper. This similarity score has a number of oddities:\n",
    "- the presence of a k-gram in a model is weighted equally, regardless of k. Each k-gram leads to two (k-1)-grams. This means that k-grams with higher k are assigned lower weights.\n",
    "- The equal weighting does not take into account whether a word is somewhat specific to one of the language distributions. In the paper, they show for example that \"prove that\" is associated with Russians and \"refute\" with the Dutch. The weighting scheme does not take such correlations into account. We could take them into account by constructing a language distribution per language and calculating the idf-tf per word present.\n",
    "\n",
    "Even though it has problems, one should note that the highest classification performance in T1-language classification is typically reached by analyzing the n-grams directly, rather than similarity scores to n-gram models. Accuracies in the range of 80-90% have been reported for this task, which is conceptually more difficult than native-language classification as it concerns classification of the native language of the writer him/herself (e.g. Jarvis/Bestgen/Pepper, Gebre/Zampierie/Wittenburg). Some interesting results:\n",
    "- In Groningen, character n-grams in the range of 8-10 alone led to very high classification accuracies. We might want to see if we can use these.\n",
    "- Somewhat unexpectedly, an ensemble of learners applied to different types of n-grams has been reported to perform better than a single learner applied to the same n-grams by itself.\n",
    "\n",
    "## Load the necessary stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the training and validation data...\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "from nltk import FreqDist, ngrams, sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold, ParameterGrid, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import pickle\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        x = pickle.load(f)\n",
    "    return(x)\n",
    "\n",
    "# Load the training data. Scramble the rows (sometimes this is important for training)\n",
    "df = pd.read_csv(\"python_data/train\",sep=\"\\t\",error_bad_lines=False,encoding=\"utf-8\")\n",
    "df = df.sample(frac=1, random_state = 54021)\n",
    "df['native'] = np.where(df['native_lang']=='EN', \"native\", \"non-native\")\n",
    "\n",
    "# Load the training data. Downsample non-English such that it is balanced.\n",
    "print(\"Loading the training and validation data...\")\n",
    "training = pd.concat([df[df.native == \"non-native\"].sample(sum(df.native == \"native\"), random_state = 1810), df[df.native==\"native\"]])\n",
    "training = training.sample(frac=1, random_state = 1318910)\n",
    "training.native = training.native.astype('category')\n",
    "\n",
    "# Load the validation data. Again, downsample such that it is balanced.\n",
    "validation = pd.read_csv(\"python_data/development\",sep=\"\\t\",error_bad_lines=False,encoding=\"utf-8\")\n",
    "validation['native'] = np.where(validation['native_lang']=='EN', \"native\", \"non-native\")\n",
    "validation = pd.concat([validation[validation.native == \"non-native\"].sample(sum(validation.native == \"native\"), random_state = 1), validation[validation.native==\"native\"]])\n",
    "validation.native = validation.native.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating tf idf\n",
    "\n",
    "We want to construct a tf-idf library for all languages separately. Hence, first derive a language distribution for all 20 languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_keys(d):\n",
    "    return (0 if not isinstance(d, dict) else len(d) + sum(sum_keys(v) for v in d.values()))\n",
    "\n",
    "def pruned_language_distribution(n, m, lowerfreqlimit, training, LANGUAGES):\n",
    "    \"\"\"Calculate the word n grams distribution up to n, the character n gram distribution up to m.\n",
    "    @n: consider k-grams up to and including n for words, part of speech tags and word sizes.\n",
    "    @m: consider k-grams up to and including m for characters. We assume m >= n.\n",
    "    @lowerfreqlimit: number below which we consider words misspellings, odd words out or unique.\n",
    "    @training: training data to retrieve the language distribution from.\n",
    "    @LANGUAGES: languages based on which we classify.\n",
    "    \"\"\"\n",
    "    \n",
    "    language_dist = {}\n",
    "\n",
    "    for language in LANGUAGES:\n",
    "        language_dist[language] = {\"words\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)])),\n",
    "                               \"tags\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)])),\n",
    "                               \"chars\": dict(zip(range(1, m+1), [FreqDist() for i in range(1, m+1)])),\n",
    "                               \"w_sizes\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)]))}\n",
    "        \n",
    "    # Iterate first over k. This is required as we need to know the full k-1 distributions to see if we should add a \n",
    "    # k-gram to the dictionary.\n",
    "    kmax = 0\n",
    "    for k in range(1, n+1):\n",
    "        for language, text, struc in training.itertuples(index=False):\n",
    "            \n",
    "            for sentence in sent_tokenize(text):\n",
    "                \n",
    "                # Get the necessary input structures for the ngrams-function. It is sentence for \"chars\".\n",
    "                token=word_tokenize(sentence) \n",
    "                wordlens = [len(word) for word in token]\n",
    "                \n",
    "                # Note, for any gram, there exist 2 subgrams of all but the first and all of the last element. Let us\n",
    "                # only update the dictionary if the total count of these subgrams exceeds the lower limit. This prevents\n",
    "                # an unnecessary combinatorial explosion.\n",
    "                for gram in ngrams(sentence,k):\n",
    "                    if k == 1: \n",
    "                        language_dist[language][\"chars\"][k][gram] += 1\n",
    "                    elif language_dist[language][\"chars\"][k-1].get(gram[1:],0)+language_dist[language][\"chars\"][k-1].get(gram[:-1],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"chars\"][k][gram] += 1\n",
    "                        \n",
    "                for gram in ngrams(token,k):\n",
    "                    if k == 1:\n",
    "                        language_dist[language][\"words\"][k][gram] += 1\n",
    "                    elif language_dist[language][\"words\"][k-1].get(gram[1:],0)+language_dist[language][\"words\"][k-1].get(gram[:-1],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"words\"][k][gram] += 1\n",
    "                        \n",
    "                for gram in ngrams(wordlens,k):\n",
    "                    if k == 1:\n",
    "                        language_dist[language][\"w_sizes\"][k][gram] += 1\n",
    "                    elif language_dist[language][\"w_sizes\"][k-1].get(gram[1:],0)+language_dist[language][\"w_sizes\"][k-1].get(gram[:-1],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"w_sizes\"][k][gram] += 1\n",
    "                        \n",
    "            # Now for the tokenized structures (tags)\n",
    "            for sentence in sent_tokenize(struc):\n",
    "                token=word_tokenize(sentence)\n",
    "                for gram in ngrams(token,k):\n",
    "                    if k == 1:\n",
    "                        language_dist[language][\"tags\"][k][gram] += 1\n",
    "                    elif language_dist[language][\"tags\"][k-1].get(gram[1:],0)+language_dist[language][\"tags\"][k-1].get(gram[:-1],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"tags\"][k][gram] += 1\n",
    "                        \n",
    "    # Also construct it for higher order k-grams for characters.\n",
    "    for k in range(n+1, m+1):\n",
    "        for language, tokenized_sents, tokenized_struc in training.itertuples(index=False):\n",
    "            for sentence in tokenized_sents:\n",
    "                for gram in ngrams(sentence,k):\n",
    "                    if language_dist[language][\"chars\"][k-1].get(gram[1:],0)+language_dist[language][\"chars\"][k-1].get(gram[:-1],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"chars\"][k][gram] += 1\n",
    "                           \n",
    "    return language_dist\n",
    "\n",
    "def construct_lodds_ratio_dict(lang_dis):\n",
    "    \n",
    "    n = len(lang_dis['EN']['words'])\n",
    "    m = len(lang_dis['EN']['chars'])\n",
    "    \n",
    "    \"\"\" \n",
    "    In this function, we want to compute the odds ratio for each of the n-grams, and return a dictionary with these values.\n",
    "    For each non-English language, we will add a pseudocount of .5 to prevent infinity.\n",
    "    \"\"\"\n",
    "    for lang in lang_dis.keys():\n",
    "        for gramtype in lang_dis[lang].keys():\n",
    "            for k in lang_dis[lang][gramtype].keys():\n",
    "                b = sum_keys(lang_dis[lang][gramtype][k])\n",
    "                d = sum_keys(lang_dis['EN'][gramtype][k])\n",
    "                for key in lang_dis[lang][gramtype][k].keys():\n",
    "                    a = lang_dis[lang][gramtype][k].get(key,0.5)\n",
    "                    c = lang_dis['EN'][gramtype][k].get(key,0.5)\n",
    "                    lang_dis[lang][gramtype][k][key] = math.log((a/b)/(c/d))\n",
    "\n",
    "                # Release the memory.\n",
    "                lang_dis[gramtype][k].clear()\n",
    "    return(lang_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to construct a language distribution for different languages. Let us not do this by downsampling to the minimum number, but rather take the same training data as previously. The log-odds ratio takes into account imbalance in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Derive the language distribution from the training data.\n",
    "print(\"Deriving the language distribution from training data...\")\n",
    "start = time.time()\n",
    "lang_dis = pruned_language_distribution(4,4,10,training[['native','text_clean','text_structure']], training.native_lang.unique())\n",
    "end = time.time()\n",
    "print(\"Language distribution constructed in {} seconds\".format(end-start))\n",
    "\n",
    "# Save it and clear it to save memory.\n",
    "save_object(lang_dis,\"trained_lang_dis_20_lang_ll10\")\n",
    "lang_dis.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the log odds ratios per entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lodds_ratio = construct_lodds_ratio_dict(lang_dis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
