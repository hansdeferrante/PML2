{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towards new features\n",
    "\n",
    "Unfortunately, parameter tuning for both Random Forests and SVM seems not to lead to large increases in classification performance. We might be able to increase, however, by extracting new features. Currently, the similarity score is calculated as in the paper. This similarity score has a number of oddities:\n",
    "- the presence of a k-gram in a model is weighted equally, regardless of k. Each k-gram leads to two (k-1)-grams. This means that k-grams with higher k are assigned lower weights.\n",
    "- The equal weighting does not take into account whether a word is somewhat specific to one of the language distributions. In the paper, they show for example that \"prove that\" is associated with Russians and \"refute\" with the Dutch. The weighting scheme does not take such correlations into account. We could take them into account by constructing a language distribution per language and calculating the idf-tf per word present.\n",
    "\n",
    "Even though it has problems, one should note that the highest classification performance in T1-language classification is typically reached by analyzing the n-grams directly, rather than similarity scores to n-gram models. Accuracies in the range of 80-90% have been reported for this task, which is conceptually more difficult than native-language classification as it concerns classification of the native language of the writer him/herself (e.g. Jarvis/Bestgen/Pepper, Gebre/Zampierie/Wittenburg). Some interesting results:\n",
    "- In Groningen, character n-grams in the range of 8-10 alone led to very high classification accuracies. We might want to see if we can use these.\n",
    "- Somewhat unexpectedly, an ensemble of learners applied to different types of n-grams has been reported to perform better than a single learner applied to the same n-grams by itself.\n",
    "\n",
    "## Load the necessary stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import gc\n",
    "from scipy.stats import norm\n",
    "from nltk import FreqDist, ngrams, sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold, ParameterGrid, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import _pickle as pickle\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        p = pickle.Pickler(output) \n",
    "        p.fast = True \n",
    "        p.dump(obj)\n",
    "        \n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        x = pickle.load(f)\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data. Scramble the rows (sometimes this is important for training). We also downsample non-native\n",
    "# English s.t. we have a 1:1 balance. This is required for a fair comparison with the work by Al-Rfou.\n",
    "print(\"Loading the training and validation data...\")\n",
    "training = pd.read_csv(\"python_data/train\",sep=\"\\t\",error_bad_lines=False,encoding=\"utf-8\")\n",
    "training = training.sample(frac=1, random_state = 54021)\n",
    "training['native'] = np.where(training['native_lang']=='EN', \"native\", \"non-native\")\n",
    "training = pd.concat([training[training.native == \"non-native\"].sample(sum(training.native == \"native\"), random_state = 1810), training[training.native==\"native\"]])\n",
    "training = training.sample(frac=1, random_state = 1318910)\n",
    "training.native = training.native.astype('category')\n",
    "\n",
    "# Load the validation data. Again, downsample such that it is balanced.\n",
    "validation = pd.read_csv(\"python_data/development\",sep=\"\\t\",error_bad_lines=False,encoding=\"utf-8\")\n",
    "validation['native'] = np.where(validation['native_lang']=='EN', \"native\", \"non-native\")\n",
    "validation = pd.concat([validation[validation.native == \"non-native\"].sample(sum(validation.native == \"native\"), random_state = 1), validation[validation.native==\"native\"]])\n",
    "validation.native = validation.native.astype('category')\n",
    "print(\"Data loaded\")\n",
    "\n",
    "# Write data to CSV. We will compute features line by line as doing it in memory is impossible for 20 languages.\n",
    "training.to_csv(\"python_data/training_tfidf\")\n",
    "validation.to_csv(\"python_data/validation_tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating tf idf\n",
    "\n",
    "We want to construct a tf-idf library for all languages separately. Hence, first derive a language distribution for all 20 languages. We end with the English language distribution. The odds ratio is there 1, trivially, but we still include the lower bound such that the measure reflects the sample size of a and c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_keys(d):\n",
    "    return (0 if not isinstance(d, dict) else len(d) + sum(sum_keys(v) for v in d.values()))\n",
    "\n",
    "def pruned_language_distribution(n, m, lowerfreqlimit, training, LANGUAGES):\n",
    "    \"\"\"Calculate the word n grams distribution up to n, the character n gram distribution up to m.\n",
    "    @n: consider k-grams up to and including n for words, part of speech tags and word sizes.\n",
    "    @m: consider k-grams up to and including m for characters. We assume m >= n.\n",
    "    @lowerfreqlimit: number below which we consider words misspellings, odd words out or unique.\n",
    "    @training: training data to retrieve the language distribution from.\n",
    "    @LANGUAGES: languages based on which we classify.\n",
    "    \"\"\"\n",
    "    \n",
    "    language_dist = {}\n",
    "\n",
    "    for language in LANGUAGES:\n",
    "        language_dist[language] = {\"words\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)])),\n",
    "                               \"tags\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)])),\n",
    "                               \"chars\": dict(zip(range(1, m+1), [FreqDist() for i in range(1, m+1)])),\n",
    "                               \"w_sizes\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)]))}\n",
    "        \n",
    "    # Iterate first over k. This is required as we need to know the full k-1 distributions to see if we should add a \n",
    "    # k-gram to the dictionary.\n",
    "    kmax = 0\n",
    "    for k in range(1, n+1):\n",
    "        for language, text, struc in training.itertuples(index=False):\n",
    "            \n",
    "            for sentence in sent_tokenize(text):\n",
    "                \n",
    "                # Get the necessary input structures for the ngrams-function. It is sentence for \"chars\".\n",
    "                token=word_tokenize(sentence) \n",
    "                wordlens = [len(word) for word in token]\n",
    "                \n",
    "                # Note, for any gram, there exist 2 subgrams of all but the first and all of the last element. Let us\n",
    "                # only update the dictionary if the total count of these subgrams exceeds the lower limit. This prevents\n",
    "                # an unnecessary combinatorial explosion.\n",
    "                for gram in ngrams(sentence,k):\n",
    "                    if k == 1: \n",
    "                        language_dist[language][\"chars\"][k][gram] += 1\n",
    "                    elif language_dist[language][\"chars\"][k-1].get(gram[1:],0)+language_dist[language][\"chars\"][k-1].get(gram[:-1],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"chars\"][k][gram] += 1\n",
    "                        \n",
    "                for gram in ngrams(token,k):\n",
    "                    if k == 1:\n",
    "                        language_dist[language][\"words\"][k][gram] += 1\n",
    "                    elif language_dist[language][\"words\"][k-1].get(gram[1:],0)+language_dist[language][\"words\"][k-1].get(gram[:-1],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"words\"][k][gram] += 1\n",
    "                        \n",
    "                for gram in ngrams(wordlens,k):\n",
    "                    if k == 1:\n",
    "                        language_dist[language][\"w_sizes\"][k][gram] += 1\n",
    "                    elif language_dist[language][\"w_sizes\"][k-1].get(gram[1:],0)+language_dist[language][\"w_sizes\"][k-1].get(gram[:-1],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"w_sizes\"][k][gram] += 1\n",
    "                        \n",
    "            # Now for the tokenized structures (tags)\n",
    "            for sentence in sent_tokenize(struc):\n",
    "                token=word_tokenize(sentence)\n",
    "                for gram in ngrams(token,k):\n",
    "                    if k == 1:\n",
    "                        language_dist[language][\"tags\"][k][gram] += 1\n",
    "                    elif language_dist[language][\"tags\"][k-1].get(gram[1:],0)+language_dist[language][\"tags\"][k-1].get(gram[:-1],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"tags\"][k][gram] += 1\n",
    "                        \n",
    "    # Also construct it for higher order k-grams for characters.\n",
    "    for k in range(n+1, m+1):\n",
    "        for language, tokenized_sents, tokenized_struc in training.itertuples(index=False):\n",
    "            for sentence in tokenized_sents:\n",
    "                for gram in ngrams(sentence,k):\n",
    "                    if language_dist[language][\"chars\"][k-1].get(gram[1:],0)+language_dist[language][\"chars\"][k-1].get(gram[:-1],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"chars\"][k][gram] += 1\n",
    "                           \n",
    "    return language_dist\n",
    "\n",
    "def construct_lodds_ratio_dict(fname, lb):\n",
    "    \"\"\" \n",
    "    In this function, we want to compute the odds ratio for each of the n-grams, and return a dictionary with these values.\n",
    "    For each non-English language, we will add a pseudocount of .5 to prevent divisions by 0. We return an approximate lower\n",
    "    bound at the alpha confidence level.\n",
    "    @fname: File to load the language distribution from\n",
    "    @alpha: level of alpha of lower bound. \n",
    "    \"\"\"\n",
    "    \n",
    "    lang_dis = load_object(fname)\n",
    "    \n",
    "    n = len(lang_dis['EN']['words'])\n",
    "    m = len(lang_dis['EN']['chars'])\n",
    "    \n",
    "    for lang in lang_dis.keys():\n",
    "        if lang == 'EN':\n",
    "            continue\n",
    "        for gramtype in lang_dis[lang].keys():            \n",
    "            for k in lang_dis[lang][gramtype].keys():\n",
    "                b = sum(lang_dis[lang][gramtype][k].values()) + .5   #Total grams in foreign language\n",
    "                d = sum(lang_dis['EN'][gramtype][k].values()) + .5   #Total grams in English\n",
    "                for key in list(lang_dis[lang][gramtype][k].keys()):\n",
    "                    \n",
    "                    # Obtain the value by pop, i.e. delete key from dictionary.\n",
    "                    a = lang_dis[lang][gramtype][k].pop(key,0) +.5   #Gram count for particular gram in foreign language\n",
    "                    c = lang_dis['EN'][gramtype][k].get(key,0) +.5   #Gram count for particular gram in English\n",
    "                    \n",
    "                    if gramtype == \"words\" and \"NNP\" in key:\n",
    "                        continue\n",
    "                    \n",
    "                    # If it occurs more often than the lower bound, set value to the lowerbound of odds ratio.\n",
    "                    if a > lb:\n",
    "                        lang_dis[lang][gramtype][k][key] = math.log((a*d)/(b*c))  # Calculate the log-odds ratio \n",
    "                    \n",
    "    # Remove English from the language dictionary.\n",
    "    lang_dis[\"EN\"].clear()\n",
    "\n",
    "    return(lang_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to construct a language distribution for different languages. Let us not do this by downsampling to the minimum number, but rather take the same training data as previously. The log-odds ratio takes into account imbalance in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive the language distribution from the training data.\n",
    "print(\"Deriving the language distribution from training data...\")\n",
    "start = time.time()\n",
    "lang_dis = pruned_language_distribution(4,4,10,training[['native_lang','text_clean','text_structure']], training.native_lang.unique())\n",
    "end = time.time()\n",
    "print(\"Language distribution constructed in {} seconds\".format(end-start))\n",
    "\n",
    "# Save it and clear it to save memory.\n",
    "save_object(lang_dis,\"trained_lang_dis_20_lang_ll10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can let our vocabulary as to which terms we want to analyze be guided by the odds ratio. This ratio is defined as $\\frac{(a/b)}{(c/d)}$. Since the count of a particular n-gram is negligible in comparison with the total number of n-grams, we can approximate b and d by the total number of n-grams in the foreign and English language distribution, respectively. We have explored using an asymptotic lower and upper bound on the odds ratio to select terms for the vocabulary used for the tokenizer. However, this has resulted in classification performance just short of 70%, which is not very good in comparison with the aggregate score. Therefore, let us be less conservative in selecting terms and allow for a bigger document-term matrix based on which we classify. To keep things somewhat robust, we put a lower bound on the gram count of 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lodds_ratio = construct_lodds_ratio_dict(\"trained_lang_dis_20_lang_ll10\", 5)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training = pd.read_csv(\"python_data/training_tfidf\")\n",
    "validation = pd.read_csv(\"python_data/validation_tfidf\")\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer as Detok\n",
    "detokenizer = Detok()\n",
    "word_gram_list = []\n",
    "char_gram_list = []\n",
    "struc_gram_list = []\n",
    "for lang in lodds_ratio.keys():\n",
    "    if lang == \"EN\":\n",
    "        continue\n",
    "    for gramtype in lodds_ratio[lang].keys():\n",
    "        for k in lodds_ratio[lang][gramtype].keys():\n",
    "            for key,v in lodds_ratio[lang][gramtype][k].items():\n",
    "                if v>math.log(6/5) or v<math.log(5/6):\n",
    "                    if gramtype == \"words\":\n",
    "                        word_gram_list.append(key)\n",
    "                    if gramtype == \"chars\":\n",
    "                        char_gram_list.append(key)\n",
    "                    if gramtype ==\"tags\":\n",
    "                        struc_gram_list.append(key)\n",
    "word_gram_list = set([detokenizer.detokenize(gram) for gram in set(word_gram_list)])\n",
    "struc_gram_list = set([detokenizer.detokenize(gram) for gram in set(struc_gram_list)])\n",
    "char_gram_list = set([''.join(gram) for gram in set(char_gram_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130249\n",
      "80360\n",
      "60487\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "lodds_ratio.clear()\n",
    "gc.collect()\n",
    "print(len(word_gram_list))\n",
    "print(len(char_gram_list))\n",
    "print(len(struc_gram_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "training = pd.read_csv(\"python_data/training_tfidf\")\n",
    "validation = pd.read_csv(\"python_data/validation_tfidf\")\n",
    "\n",
    "# create the transform for words, characters and structure.\n",
    "word_vectorizer = CountVectorizer(ngram_range=(1, 4), vocabulary = word_gram_list, lowercase=False)\n",
    "struc_vectorizer = CountVectorizer(ngram_range=(1, 4), vocabulary = struc_gram_list, lowercase=False)\n",
    "char_vectorizer = CountVectorizer(ngram_range=(1,4), vocabulary = char_gram_list, analyzer=\"char\", lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vector = word_vectorizer.transform(training[\"text_clean\"])\n",
    "struc_vector = struc_vectorizer.transform(training[\"text_structure\"])\n",
    "char_vector = char_vectorizer.transform(training[\"text_clean\"])\n",
    "\n",
    "from scipy import sparse\n",
    "sparse.save_npz(\"word_vector\",word_vector)\n",
    "sparse.save_npz(\"struc_vector\",struc_vector)\n",
    "sparse.save_npz(\"char_vector\",char_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "word_vector = sparse.load_npz(\"word_vector.npz\")\n",
    "struc_vector = sparse.load_npz(\"struc_vector.npz\")\n",
    "char_vector = sparse.load_npz(\"char_vector.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64928386649\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(char_vector, training[\"native\"])\n",
    "validation_char_vector = char_vectorizer.transform(validation[\"text_clean\"])\n",
    "predicted = clf.predict(validation_char_vector)\n",
    "accuracy = 1-sum(predicted != validation.native)/len(predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.694775695627\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(word_vector, training[\"native\"])\n",
    "validation_word_vector = word_vectorizer.transform(validation[\"text_clean\"])\n",
    "predicted = clf.predict(validation_word_vector)\n",
    "accuracy = 1-sum(predicted != validation.native)/len(predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.512208972175\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(struc_vector, training[\"native\"])\n",
    "validation_struc_vector = struc_vectorizer.transform(validation[\"text_clean\"])\n",
    "predicted = clf.predict(validation_struc_vector)\n",
    "accuracy = 1-sum(predicted != validation.native)/len(predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.720991860685\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss=\"hinge\",penalty=\"l2\",alpha=1e-4, random_state=42, max_iter=50, tol=None)\n",
    "clf.fit(word_vector, training.native)\n",
    "predicted = clf.predict(validation_word_vector)\n",
    "accuracy = 1-sum(predicted != validation.native)/len(predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-14f3562c252b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hinge\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"l2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnative\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_char_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnative\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m    584\u001b[0m                          \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                          \u001b[0mcoef_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoef_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mintercept_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                          sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         array = _ensure_sparse_format(array, accept_sparse, dtype, copy,\n\u001b[0;32m--> 431\u001b[0;31m                                       force_all_finite)\n\u001b[0m\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# convert dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0mspmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchanged_format\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;31m# force copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/sparse/data.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deduped_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mastype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(loss=\"hinge\",penalty=\"l2\",alpha=1e-4, random_state=42, max_iter=50, tol=None)\n",
    "clf.fit(char_vector, training.native)\n",
    "predicted = clf.predict(validation_char_vector)\n",
    "accuracy = 1-sum(predicted != validation.native)/len(predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-0dc29a4d99a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hinge\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"l2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruc_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnative\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvalidation_struc_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruc_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_clean\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_struc_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnative\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m    584\u001b[0m                          \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                          \u001b[0mcoef_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoef_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mintercept_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                          sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         array = _ensure_sparse_format(array, accept_sparse, dtype, copy,\n\u001b[0;32m--> 431\u001b[0;31m                                       force_all_finite)\n\u001b[0m\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# convert dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0mspmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchanged_format\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;31m# force copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/sparse/data.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deduped_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mastype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(loss=\"hinge\",penalty=\"l2\",alpha=1e-4, random_state=42, max_iter=50, tol=None)\n",
    "clf.fit(struc_vector, training.native)\n",
    "validation_struc_vector = struc_vectorizer.transform(validation[\"text_clean\"])\n",
    "predicted = clf.predict(validation_struc_vector)\n",
    "accuracy = 1-sum(predicted != validation.native)/len(predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Compute similarity scores against log-odds dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we tried to compute a similarity score analogous to the one in the replication based on all 20 languages and odds ratios. This turns out to work pretty badly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_similarity_score_sum(dis_ngramdic, gramlist):\n",
    "    \"\"\" This function computes the similarity scores for a comment based on the corresponding k-grams.\n",
    "    Note that the comment is already tokenized into sentences.\n",
    "    @dis_ngramdic: ngram dictionary as constructed by language_distribution for particular k.\n",
    "    @gramlist: list of kgrams\n",
    "    \"\"\"\n",
    "    score=0\n",
    "    if gramlist:\n",
    "        for gram in gramlist:\n",
    "            score += dis_ngramdic.get(gram,1)\n",
    "    return score\n",
    "\n",
    "colnames = None\n",
    "\n",
    "\n",
    "def compute_all_features(lang_dis, original_text, clean_text, structure_text):\n",
    "    \"\"\" This function compares the sentences and structure to each of the languages distributions. It returns\n",
    "    similarity scores to each language model. Also included are other features, such as the number of sentences\n",
    "    per text, etc.\n",
    "    @lang_dis: Language distribution of n-grams.\n",
    "    @clean_text: Text with proper nouns and demonyms substituted\n",
    "    @structure_text: PoS structure retrieved by SENNA.\n",
    "    \"\"\"\n",
    "    simscoredict=dict()\n",
    "    \n",
    "    # For each gramtype, first construct the list of which we can make n-grams.\n",
    "    words_ps = list(word_tokenize(clean_text))\n",
    "    struc_ps = list(word_tokenize(structure_text))\n",
    "    wordlens_ps = [len(word) for word in word_tokenize(original_text) if word.isalpha()]\n",
    "    \n",
    "    # Now we should construct k-gram lists for each k and return the score. Let us store all grams in \n",
    "    for gramtype in lang_dis[list(lang_dis.keys())[0]].keys():\n",
    "        \n",
    "        # Select appropriate data type.\n",
    "        if gramtype == \"tags\":\n",
    "            ps = struc_ps\n",
    "        elif gramtype ==\"words\":\n",
    "            ps = words_ps\n",
    "        elif gramtype == \"w_sizes\":\n",
    "            ps = wordlens_ps\n",
    "        elif gramtype == \"chars\":\n",
    "            ps = clean_text\n",
    "            \n",
    "        seq_len = len(ps) if len(ps) != 0 else 1\n",
    "        \n",
    "        # For each k, feed the ngrams function into the compute_similarity_score function. \n",
    "        for k in range(1,len(lang_dis[list(lang_dis.keys())[0]][gramtype])+1):\n",
    "            for lang in lang_dis.keys():\n",
    "                simscoredict[lang+'_'+gramtype+'_'+str(k)] = compute_similarity_score_sum(lang_dis[lang][gramtype][k], ngrams(ps,k))/seq_len\n",
    "    \n",
    "    # Set the other features they use in the paper.\n",
    "    simscoredict[\"num_sentences\"] = len(list(sent_tokenize(clean_text)))\n",
    "    simscoredict[\"num_words\"] = len(wordlens_ps)\n",
    "    simscoredict[\"avg_wordlength\"] = sum(wordlens_ps)/len(wordlens_ps)\n",
    "        \n",
    "    global colnames\n",
    "    if colnames == None:\n",
    "        colnames = list(simscoredict.keys())\n",
    "            \n",
    "    return simscoredict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "print(\"Starting computing features against each language distribution\")\n",
    "start=time.time()\n",
    "\n",
    "with open(\"python_data/training_tfidf\") as infile, open(\"python_data/training_with_features_tfidf\",\"w\") as outfile:\n",
    "    # Open csv reader and writer.\n",
    "    r =  csv.reader(infile); w = csv.writer(outfile); next(r)\n",
    "    header_written = False\n",
    "    for row in r:\n",
    "        features = compute_all_features(lodds_ratio,row[2],row[4],row[5])\n",
    "        if not header_written:\n",
    "            w.writerow(['','native_lang','native','level']+colnames)\n",
    "            header_written = True\n",
    "        w.writerow([row[0], row[1], row[3], row[6]]+ list(features))\n",
    "\n",
    "with open(\"python_data/validation_tfidf\") as infile, open(\"python_data/validation_with_features_tfidf\",\"w\") as outfile:\n",
    "    # Open csv reader and writer.\n",
    "    r =  csv.reader(infile); w = csv.writer(outfile); next(r)\n",
    "    header_written = False\n",
    "    for row in r:\n",
    "        features = compute_all_features(lodds_ratio,row[2],row[4],row[5])\n",
    "        if not header_written:\n",
    "            w.writerow(['','native_lang','native','level']+colnames)\n",
    "            header_written = True\n",
    "        w.writerow([row[0], row[1], row[3], row[6]]+ list(features))\n",
    "\n",
    "print(\"Features calculated in {} seconds\".format(time.time()-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
