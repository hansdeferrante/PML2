{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will investigate how good a score can be achieved using character 10-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import gc\n",
    "from scipy import stats\n",
    "from nltk import FreqDist, ngrams, sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from scipy import sparse\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold, ParameterGrid, RandomizedSearchCV, train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import _pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        p = pickle.Pickler(output) \n",
    "        p.fast = True \n",
    "        p.dump(obj)\n",
    "        \n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        x = pickle.load(f)\n",
    "    return(x)\n",
    "\n",
    "def sum_keys(d):\n",
    "    return (0 if not isinstance(d, dict) else len(d) + sum(sum_keys(v) for v in d.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data & tree construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing a dictionary of all 10-grams is intractable on an 8GB RAM computer. To make things feasible we have to trim the tree quite heavily. Imposing a lower limit in the count of grams still yields an intractable tree (some 20 million unique keys). Hence, we write a function to trim the tree. We do this based on a few ideas, including (i) imposing a stricter lower limit on k-grams with low k, (ii) selecting words that distinguish native from non-native using odds ratios and (iii) removing branches of the tree that add no information. \n",
    "\n",
    "Under (iii) we understand branches that stem from a unique internal node. E.g. \"cyclop\" corresponds almost uniquely to \"encyclopedia\" and adding n-grams such as \"ncyclop\" doesn't add further information to the tree. We take care of this trimming in a second function `construct_log_odds_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def char_distribution(m, lowerfreqlimit, training, LANGUAGES):\n",
    "    \"\"\"Calculate the char m grams distribution.\n",
    "    @m: consider k-grams up to and including m for characters.\n",
    "    @lowerfreqlimit: number below which we consider grams near-unique and irrelevant\n",
    "    @training: training data to retrieve the language distribution from.\n",
    "    @LANGUAGES: languages based on which we classify. Either native languages or \"non-native\"/\"native\" divide is possible.\n",
    "    \"\"\"\n",
    "    \n",
    "    char_dist = {}\n",
    "\n",
    "    for language in LANGUAGES:\n",
    "        char_dist[language] = dict(zip(range(1, m+1), [FreqDist() for i in range(1, m+1)]))\n",
    "    \n",
    "    for k in range(1, m+1):\n",
    "        for language, text in training.itertuples(index=False):\n",
    "            for sentence in sent_tokenize(text):\n",
    "                \n",
    "                # Note, for any gram, there exist 2 subgrams of all but the first and all of the last element. Let us\n",
    "                # only update the dictionary if the total count of these subgrams exceeds the lower limit. This prevents\n",
    "                # an unnecessary combinatorial explosion.                \n",
    "                \n",
    "                for gram in ngrams(sentence,k):\n",
    "                    if k == 1: \n",
    "                        char_dist[language][k][gram] += 1\n",
    "                    elif char_dist[language][k-1].get(gram[1:],0)+char_dist[language][k-1].get(gram[:-1],0) > 2*lowerfreqlimit:\n",
    "                        char_dist[language][k][gram] += 1\n",
    "                        \n",
    "        print(\"Completed counting all {}-grams\".format(k))\n",
    "                                               \n",
    "    return char_dist\n",
    "\n",
    "def construct_lodds_ratio_dict(fname, alpha, lbo, lbc, fc):\n",
    "    \"\"\" \n",
    "    In this function, we want to compute the odds ratio for each of the n-grams, and return a dictionary with these values.\n",
    "    For each non-English language, we will add a pseudocount of .5 to prevent divisions by 0. We return an approximate lower\n",
    "    bound at the alpha confidence level.\n",
    "    @fname: File to load the language distribution from\n",
    "    @alpha: Alpha on the log-odds ratio between we will not \n",
    "    @lbo: lower bound for lb on log odds ratio. Upperbound is its inverse (assume symmetry).\n",
    "    @lbc: lower bound on the count. We want to be less harsh for higher k, so we select on lbc/k\n",
    "    @fc: factor how much the children of a gram may account for them for the node to be superfluous.\n",
    "         Note: unique k-grams have two children. Thus, factor 2 is expected for unique grams. Reasonable\n",
    "         values thus order 2-4.\n",
    "    \"\"\"\n",
    "    \n",
    "    lang_dis = load_object(fname)\n",
    "    z = stats.norm.ppf(1-alpha)\n",
    "    m = len(lang_dis['EN'])\n",
    "    \n",
    "    disc = 0\n",
    "    for lang in lang_dis.keys():\n",
    "        if lang == 'EN':\n",
    "            continue       \n",
    "    \n",
    "        # First let us prune the tree, i.e. remove any superfluous branches.\n",
    "        for k in sorted(list(lang_dis[lang].keys()), reverse=True):\n",
    "            if k == 1:\n",
    "                continue\n",
    "                                            \n",
    "            # Prune the tree by removing elements that are superfluous. Let us define them as superfluous\n",
    "            # if they represent 95% of observations of the parent node.\n",
    "            for key in list(lang_dis[lang][k].keys()):\n",
    "                                \n",
    "                # Remove the key and get the count. If superfluous, delete but add to remainder.\n",
    "                a = lang_dis[lang][k].pop(key,0)\n",
    "                                \n",
    "                if lang_dis[lang][k-1].get(key[1:],0) + lang_dis[lang][k-1].get(key[:-1],0) < fc*a:\n",
    "                    disc+=1\n",
    "                    lang_dis[lang][k][\"REMAINDER\"] += a\n",
    "                else:\n",
    "                    lang_dis[lang][k][key] = a\n",
    "                    \n",
    "        # Now prune the tree further with log-odds ratios.\n",
    "        for k in lang_dis[lang].keys():\n",
    "            \n",
    "            b = sum(lang_dis[lang][k].values()) + .5   #Total grams in foreign language\n",
    "            d = sum(lang_dis['EN'][k].values()) + .5   #Total grams in English\n",
    "            rem = lang_dis[lang][k].pop(\"REMAINDER\",0)\n",
    "            \n",
    "            for key in list(lang_dis[lang][k].keys()):\n",
    "                    \n",
    "                # Obtain the value by pop, i.e. delete key from dictionary.\n",
    "                a = lang_dis[lang][k].pop(key,0) +.5   #Gram count for particular gram in foreign language\n",
    "                c = lang_dis['EN'][k].get(key,0) +.5   #Gram count for particular gram in English\n",
    "                \n",
    "                # Log odds ratio\n",
    "                log_odds_ratio = math.log((a/b)/(c/d))\n",
    "                alpha_lim = z*math.sqrt(1/a+1/b+1/c+1/d)\n",
    "\n",
    "                # Lower bound must be greater than limit, upper bound lower than lower limit.\n",
    "                if log_odds_ratio - alpha_lim > math.log(lbo) or log_odds_ratio - alpha_lim < math.log(1/lbo):\n",
    "                    if a > lbc/k:\n",
    "                        lang_dis[lang][k][key] = a - .5\n",
    "                    else:\n",
    "                        lang_dis[lang][k][\"REMAINDER\"] += a\n",
    "                else:\n",
    "                    lang_dis[lang][k][\"REMAINDER\"] += a\n",
    "                    \n",
    "    print(\"Discarded {} keys that were superfluous\".format(disc))\n",
    "            \n",
    "    # Remove English from the language dictionary.\n",
    "    lang_dis[\"EN\"].clear()\n",
    "\n",
    "    return(lang_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the training and validation data...\n"
     ]
    }
   ],
   "source": [
    "# Load same data as in other notebooks.\n",
    "df = pd.read_csv(\"python_data/train\",sep=\"\\t\",error_bad_lines=False,encoding=\"utf-8\")\n",
    "df = df.sample(frac=1, random_state = 54021)\n",
    "df['native'] = np.where(df['native_lang']=='EN', \"native\", \"non-native\")\n",
    "\n",
    "# Select training samples.\n",
    "print(\"Loading the training and validation data...\")\n",
    "training = pd.concat([df[df.native == \"non-native\"].sample(sum(df.native == \"native\"), random_state = 1810), df[df.native==\"native\"]])\n",
    "training = training.sample(frac=1, random_state = 1318910)\n",
    "training.native = training.native.astype('category')\n",
    "df = df.iloc[0:0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting grams to prevent memory issues.\n",
    "\n",
    "Selecting grams based on log-odds ratios and then using these same grams as features for learning from training data per se may lead to overfitting. Also, it is intractable to select the most important features on all training data. Hence, we first split training data into two sets. From the first set, we will construct a log-odds dictionary and select interesting grams. We will use this list on a second set selecting the most important features. This hopefully results in a robust set of grams based on which we can train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into two sets. First will be used to construct a log-odds dictionary, second to evaluate selected terms.\n",
    "X_in_sample, X_out_sample = train_test_split(training[['native_lang','text_clean']], test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the character distribution\n",
      "Completed counting all 1-grams\n",
      "Completed counting all 2-grams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hans/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed counting all 3-grams\n",
      "Completed counting all 4-grams\n",
      "Completed counting all 5-grams\n",
      "Completed counting all 6-grams\n",
      "Completed counting all 7-grams\n",
      "Completed counting all 8-grams\n",
      "Completed counting all 9-grams\n",
      "Completed counting all 10-grams\n",
      "Character distribution constructed with 22416133 unique grams\n"
     ]
    }
   ],
   "source": [
    "# Create a character distribution from the training set. Set lowerlimit to 15, otherwise it won't save.\n",
    "print(\"Training the character distribution\")\n",
    "char_dis = char_distribution(10, 15, X_in_sample, training.native_lang.unique())\n",
    "\n",
    "# Clear all stuff in memory which is not the character distribution so that Pickling doesn't fail...\n",
    "X_in_sample = X_in_sample.iloc[0:0]\n",
    "X_out_sample = X_out_sample.iloc[0:0]\n",
    "training = training.iloc[0:0]\n",
    "gc.collect()\n",
    "\n",
    "# Pickle the object.\n",
    "save_object(char_dis,\"trained_char_dis_10\")\n",
    "print(\"Character distribution constructed with {} unique grams\".format(sum_keys(char_dis)))\n",
    "char_dis.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discarded 1069412 keys that were superfluous\n",
      "There are 308177 grams remaining which will be the basis for the initial vocabulary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct the log-odds dictionary. From this dictionary, retrieve the important grams.\n",
    "lodds_ratio = construct_lodds_ratio_dict(\"trained_char_dis_10\", 0.01, 11/10, 100, 4)\n",
    "char_gram_list = []\n",
    "for lang in lodds_ratio.keys():\n",
    "    if lang == \"EN\":\n",
    "        continue\n",
    "    for k in lodds_ratio[lang].keys():\n",
    "        for key in lodds_ratio[lang][k].keys():\n",
    "            char_gram_list.append(key)\n",
    "char_gram_list = set(char_gram_list)\n",
    "char_gram_list = [''.join(gram) for gram in char_gram_list]\n",
    "print(\"There are {} grams remaining which will be the basis for the initial vocabulary\".format(len(char_gram_list)))\n",
    "\n",
    "lodds_ratio.clear()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of character grams is still of an intractable size. Let us select important features from it with SelectFromModel. We need a sensible way to select features. Note that the features here essentially reduce to words/misspellings. Random forests seems to be a sensible way to select features. Let us do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing the out-of-sample document-term matrix\n",
      "Training with out-of-sample document-term matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=10, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(141921)\n",
    "print(\"Constructing the out-of-sample document-term matrix\")\n",
    "char_vectorizer = CountVectorizer(ngram_range=(1,10), vocabulary = char_gram_list, analyzer=\"char\", lowercase=False, dtype=np.float64)\n",
    "char_vector = char_vectorizer.transform(X_out_sample.text_clean)\n",
    "print(\"Training with out-of-sample document-term matrix\")\n",
    "X_out_sample['native'] = np.where(X_out_sample['native_lang']=='EN', \"native\", \"non-native\")\n",
    "rf = RandomForestClassifier(n_estimators=500, min_samples_leaf = 10, n_jobs = -1)\n",
    "rf.fit(char_vector, X_out_sample.native)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the filtered list is 41790\n"
     ]
    }
   ],
   "source": [
    "filtered_char_list = []\n",
    "for gram, boolean in zip(char_gram_list, SelectFromModel(rf, threshold=1e-6, prefit = True).get_support()):\n",
    "    if boolean:\n",
    "        filtered_char_list.append(gram)\n",
    "print(\"Length of the filtered list is {}\".format(len(filtered_char_list)))\n",
    "\n",
    "save_object(filtered_char_list, \"selected_subset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart python & reload the selected features. We use these to construct the DTM for validation and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing the document-term matrix for 60% of training data\n",
      "Constructing DTM for validation data\n"
     ]
    }
   ],
   "source": [
    "# Select training samples.\n",
    "filtered_char_list = load_object(\"selected_subset\")\n",
    "\n",
    "print(\"Constructing the document-term matrix for 60% of training data\")\n",
    "training = training.sample(frac=0.6, random_state=45)\n",
    "char_vectorizer = CountVectorizer(ngram_range=(1,10), vocabulary = filtered_char_list, analyzer=\"char\", lowercase=False)\n",
    "char_vector = char_vectorizer.transform(training.text_clean)\n",
    "training.to_csv(\"python_data/training_60%_sample\")\n",
    "training_label = training.native\n",
    "training = training.iloc[0:0]\n",
    "\n",
    "# Load validation data.\n",
    "validation = pd.read_csv(\"python_data/development\",sep=\"\\t\",error_bad_lines=False,encoding=\"utf-8\")\n",
    "validation['native'] = np.where(validation['native_lang']=='EN', \"native\", \"non-native\")\n",
    "validation = pd.concat([validation[validation.native == \"non-native\"].sample(sum(validation.native == \"native\"), random_state = 1), validation[validation.native==\"native\"]])\n",
    "validation.native = validation.native.astype('category')\n",
    "\n",
    "# Construct char vector for validation data.\n",
    "print(\"Constructing DTM for validation data\")\n",
    "validation_char_vector = char_vectorizer.transform(validation.text_clean)\n",
    "validation_label = validation.native\n",
    "validation = validation.iloc[0:0]\n",
    "sparse.save_npz(\"char_10_vector\",char_vector)\n",
    "sparse.save_npz(\"val_char_10_vector\",validation_char_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload again. Now load the DTMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data and grams.\n",
    "training = pd.read_csv(\"python_data/training_60%_sample\")\n",
    "validation = pd.read_csv(\"python_data/validation_tfidf\")\n",
    "training_label = training.native\n",
    "validation_label = validation.native\n",
    "validation = validation.iloc[0:0]\n",
    "training = training.iloc[0:0]\n",
    "gc.collect()\n",
    "\n",
    "# Load DTMs\n",
    "training_grams = sparse.load_npz(\"char_10_vector.npz\")\n",
    "validation_grams = sparse.load_npz(\"val_char_10_vector.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Naive Bayes estimator\n",
      "Naive Bayes accuracy: 0.6472017161966055\n",
      "Classifying with SVM with gradient descent gives accuracy of 68.17149346961952%\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naive Bayes\n",
    "print(\"Training the Naive Bayes estimator\")\n",
    "clf = MultinomialNB(alpha=0.01).fit(training_grams, training_label)\n",
    "predicted_NB = clf.predict(validation_grams)\n",
    "accuracy = 1-sum(predicted_NB != validation_label)/len(predicted_NB)\n",
    "print(\"Naive Bayes accuracy: {}\".format(accuracy))\n",
    "clf = SGDClassifier(loss=\"hinge\",penalty=\"l2\",alpha=1e-4, random_state=42, max_iter=500, tol=None)\n",
    "clf.fit(training_grams, training_label)\n",
    "predicted_SVM = clf.predict(validation_grams)\n",
    "accuracy = 1-sum(predicted_SVM != validation_label)/len(predicted_SVM)\n",
    "print(\"Classifying with SVM with gradient descent gives accuracy of {}%\".format(accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
