{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and brief description data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing packages we need and loading the data. We will use pandas to store the data. This structure modeled on Worksheet 3 and the scripts from Wikitalk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "from nltk import FreqDist, ngrams, sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the training data. Scramble the rows (sometimes this is important for training)\n",
    "df = pd.read_csv(\"python_data/train\",sep=\"\\t\",error_bad_lines=False,encoding=\"utf-8\",nrows=50000)\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "# Load some datasets we will need later on. E.g. English stopwords.\n",
    "eng_stopwords = stopwords.words('english') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first take a brief look at the data. We see that there are approximately 323K comments in the training data. For each comment, we have the native language of the writer (native_lang), the self-reported level of the english speaker (native, 1, 2, 3, 4, 5 or unknown), the original text of the comment (text_original), the text with demonyms and proper nouns replaced by PoS tags (text_clean), and finally the structure of the text reported by SENNA (text_structure). Of the 323K comments, about 110K comments are from native speakers. Thus, the training data seems sufficiently balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>native_lang</th>\n",
       "      <th>text_original</th>\n",
       "      <th>level_english</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_structure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>20</td>\n",
       "      <td>49603</td>\n",
       "      <td>7</td>\n",
       "      <td>49600</td>\n",
       "      <td>49546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>EN</td>\n",
       "      <td>is being used on this article. I notice the im...</td>\n",
       "      <td>N</td>\n",
       "      <td>is being used on this article. I notice the im...</td>\n",
       "      <td>VBZ VBG VBN IN DT NN . PRP VBP DT NN NN VBZ IN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>17172</td>\n",
       "      <td>109</td>\n",
       "      <td>17172</td>\n",
       "      <td>109</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       native_lang                                      text_original  \\\n",
       "count        50000                                              50000   \n",
       "unique          20                                              49603   \n",
       "top             EN  is being used on this article. I notice the im...   \n",
       "freq         17172                                                109   \n",
       "\n",
       "       level_english                                         text_clean  \\\n",
       "count          50000                                              50000   \n",
       "unique             7                                              49600   \n",
       "top                N  is being used on this article. I notice the im...   \n",
       "freq           17172                                                109   \n",
       "\n",
       "                                           text_structure  \n",
       "count                                               50000  \n",
       "unique                                              49546  \n",
       "top     VBZ VBG VBN IN DT NN . PRP VBP DT NN NN VBZ IN...  \n",
       "freq                                                  109  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     50000\n",
       "unique       20\n",
       "top          EN\n",
       "freq      17172\n",
       "Name: native_lang, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.native_lang.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>native_lang</th>\n",
       "      <th>text_original</th>\n",
       "      <th>level_english</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_structure</th>\n",
       "      <th>native</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>20</td>\n",
       "      <td>49603</td>\n",
       "      <td>7</td>\n",
       "      <td>49600</td>\n",
       "      <td>49546</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>EN</td>\n",
       "      <td>is being used on this article. I notice the im...</td>\n",
       "      <td>N</td>\n",
       "      <td>is being used on this article. I notice the im...</td>\n",
       "      <td>VBZ VBG VBN IN DT NN . PRP VBP DT NN NN VBZ IN...</td>\n",
       "      <td>non-native</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>17172</td>\n",
       "      <td>109</td>\n",
       "      <td>17172</td>\n",
       "      <td>109</td>\n",
       "      <td>109</td>\n",
       "      <td>32828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       native_lang                                      text_original  \\\n",
       "count        50000                                              50000   \n",
       "unique          20                                              49603   \n",
       "top             EN  is being used on this article. I notice the im...   \n",
       "freq         17172                                                109   \n",
       "\n",
       "       level_english                                         text_clean  \\\n",
       "count          50000                                              50000   \n",
       "unique             7                                              49600   \n",
       "top                N  is being used on this article. I notice the im...   \n",
       "freq           17172                                                109   \n",
       "\n",
       "                                           text_structure      native  \n",
       "count                                               50000       50000  \n",
       "unique                                              49546           2  \n",
       "top     VBZ VBG VBN IN DT NN . PRP VBP DT NN NN VBZ IN...  non-native  \n",
       "freq                                                  109       32828  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['native'] = np.where(df['native_lang']=='EN', \"native\", \"non-native\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replication Al-'Rfou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper on which we've based our project on uses similarity scores to word and character n-gram models as the features for subsequent classification. Let us embark too on construction of such models. However, other literature has shown that for character n-grams, increasing n seems to enhance classifcation. Thus, we will construct models for up to 10 n-gram models.\n",
    "\n",
    "We can start by tokenizing sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df['tokenized_sents'] = df.apply(lambda row: sent_tokenize(row['text_clean']), axis=1)\n",
    "#df['tokenized_struc'] = df.apply(lambda row: sent_tokenize(row['text_structure']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we derive distributions of language use of n-grams among native and non-native speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def language_distribution(n, m, lowerfreqlimit, training, LANGUAGES):\n",
    "    \"\"\"Calculate the word n grams distribution up to n, the character n gram distribution up to m.\n",
    "    @n: consider k-grams up to and including n for words, part of speech tags and word sizes.\n",
    "    @m: consider k-grams up to and including m for characters.\n",
    "    @lowerfreqlimit: number below which we consider words misspellings, odd words out or unique.\n",
    "    @training: training data to retrieve the language distribution from.\n",
    "    @LANGUAGES: languages based on which we classify.\n",
    "    \"\"\"\n",
    "    \n",
    "    language_dist = {}\n",
    "\n",
    "    for language in LANGUAGES:\n",
    "        language_dist[language] = {\"words\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)])),\n",
    "                               \"tags\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)])),\n",
    "                               \"chars\": dict(zip(range(1, m+1), [FreqDist() for i in range(1, m+1)])),\n",
    "                               \"w_sizes\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)]))}\n",
    "    \n",
    "    # First fill up 1-grams for words. This is necessary since we should ignore unique words for\n",
    "    # higher order n-grams or we will get combinatorial explosions with result in memory errors.\n",
    "    \n",
    "    for language, tokenized_sents, tokenized_struc in training.itertuples(index=False):\n",
    "        for sentence in tokenized_sents:\n",
    "            token=word_tokenize(sentence)\n",
    "            wordlens = [len(word) for word in token]  \n",
    "            language_dist[language][\"words\"][1].update(ngrams(token,1))\n",
    "            language_dist[language][\"w_sizes\"][1].update(ngrams(wordlens,1))\n",
    "            \n",
    "    # Now do it for all higher order k-grams, ignoring word k-grams that occur at most 1 time.\n",
    "       \n",
    "    unique_words = set(filter(lambda x: language_dist[\"non-native\"][\"words\"][1][x] > lowerfreqlimit, language_dist[\"non-native\"][\"words\"][1]))\n",
    "    \n",
    "    for language, tokenized_sents, tokenized_struc in training.itertuples(index=False):\n",
    "    \n",
    "        # Construct n grams counts from the tokenized sentences.\n",
    "        for sentence in tokenized_sents:\n",
    "            token = word_tokenize(sentence)     \n",
    "            wordlens = [len(word) for word in token if word.isalpha()]   \n",
    "                        \n",
    "            # Start from index 2 since 1 is already filled.\n",
    "            for k in range(2,n+1):  \n",
    "                \n",
    "                # Allow all word length k-grams.\n",
    "                language_dist[language][\"w_sizes\"][k].update(ngrams(wordlens,k))\n",
    "                \n",
    "                # Remove any ngrams that have words which are unique.\n",
    "                allowed_ngrams=[]                \n",
    "                for ngram in ngrams(token,k):\n",
    "                    allowed = True\n",
    "                    for word in ngram:\n",
    "                        if word in unique_words:\n",
    "                            allowed=False \n",
    "                    if allowed:\n",
    "                        allowed_ngrams.append(ngram)              \n",
    "                \n",
    "                #allowed_ngrams = [ngram for ngram in ngrams(token,k) if all(word not in unique_words for word in ngram)]\n",
    "                \n",
    "                language_dist[language][\"words\"][k].update(FreqDist(allowed_ngrams))             \n",
    "                \n",
    "                \n",
    "        # Construct n gram counts from sentences tokenized based on structure.\n",
    "        for sentence in tokenized_struc:\n",
    "            token = word_tokenize(sentence)\n",
    "            for k in range(1,n+1):\n",
    "                language_dist[language][\"tags\"][k].update(ngrams(token,k))\n",
    "\n",
    "        # Construct character m-grams for tokenized sentences.\n",
    "        for sentence in tokenized_sents:\n",
    "            for k in range(1,m+1):\n",
    "                language_dist[language][\"chars\"][k].update(ngrams(sentence,k))\n",
    "    \n",
    "    return language_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the distribution of grams over different languages, we can compute similarity scores. Note, for each language one can compute similarity score against each k-gram model. Thus, this results here in 2*(4+4+4+10) = 2x22 = 44 features. The paper mentiones that the scores are calculated as the sum of the 2-logs of counts in the model. Inspection of the scripts, however, show that this is not the case: this 2-log is normalized by the length of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_similarity_score(dis_ngramdic, gramlist):\n",
    "    \"\"\" This function computes the similarity scores for a comment based on the corresponding k-grams.\n",
    "    Note that the comment is already tokenized into sentences.\n",
    "    @dis_ngramdic: ngram dictionary as constructed by language_distribution for particular k.\n",
    "    @gramlist: list of kgrams\n",
    "    \"\"\"\n",
    "    score=0\n",
    "    if gramlist:\n",
    "        for gram in gramlist:\n",
    "            score += math.log2(dis_ngramdic.get(gram,1))\n",
    "    return score\n",
    "\n",
    "colnames = None\n",
    "\n",
    "def compute_all_features(lang_dis, tokenized_sent, tokenized_struc):\n",
    "    \"\"\" This function compares the tokenized sentences and tokenized structure to each of the languages distributions.\n",
    "    It returns similarity scores to each language model. Also included are other features, such as the number of sentences\n",
    "    per \n",
    "    @lang_dis: Language distribution of n-grams.\n",
    "    @tokenized_sents: sentences tokenized by nltk.ngrams\n",
    "    @tokenized_struc: PoS structure retrieved by SENNA, tokenized by nltk.ngrams.``\n",
    "    \"\"\"\n",
    "    simscoredict=dict()\n",
    "\n",
    "    # For each gramtype, first construct the list of which we can make n-grams.\n",
    "    wordlens_ps = []\n",
    "    words_ps = []\n",
    "    struc_ps = []\n",
    "    \n",
    "    for sentence in tokenized_sent:\n",
    "        wordlens_ps.append([len(word) for word in word_tokenize(sentence) if word.isalpha()])\n",
    "        words_ps.append([word for word in word_tokenize(sentence)])\n",
    "    \n",
    "    for sentence in tokenized_struc:\n",
    "        struc_ps.append([word for word in word_tokenize(sentence)])\n",
    "    \n",
    "    # Now we should construct k-gram lists for each k and return the score. Let us store all grams in \n",
    "    for gramtype in lang_dis[list(lang_dis.keys())[0]].keys():\n",
    "        \n",
    "        # Select appropriate data type.\n",
    "        if gramtype == \"tags\":\n",
    "            ps = struc_ps\n",
    "        elif gramtype ==\"words\":\n",
    "            ps = words_ps\n",
    "        elif gramtype == \"w_sizes\":\n",
    "            ps = wordlens_ps\n",
    "        else:\n",
    "            ps = tokenized_struc\n",
    "            \n",
    "        seq_len = sum([len(item) for item in ps])\n",
    "            \n",
    "        # Construct for each k a gramlist. \n",
    "        for k in range(1,len(lang_dis[list(lang_dis.keys())[0]][gramtype])+1):\n",
    "            \n",
    "            kgramlist = list(set([gram for sentence in ps for gram in ngrams(sentence, k)]))\n",
    "            \n",
    "            for lang in lang_dis.keys():\n",
    "                simscoredict[lang+'_'+gramtype+'_'+str(k)]= compute_similarity_score(lang_dis[lang][gramtype][k], kgramlist)/seq_len \n",
    "        \n",
    "    # Set the other features they use in the paper.\n",
    "    simscoredict[\"num_sentences\"] = len(tokenized_sent) if isinstance(tokenized_sent, list) else 0\n",
    "    simscoredict[\"num_words\"] = sum([len(wc) for wc in wordlens_ps])\n",
    "    simscoredict[\"avg_wordlength\"] = sum([sum(word) for word in wordlens_ps])/simscoredict[\"num_words\"]\n",
    "    \n",
    "    global colnames\n",
    "    if colnames == None:\n",
    "        colnames = list(simscoredict.keys())\n",
    "            \n",
    "    return simscoredict.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al Rfou reports having used some different features too. It is not entirely clear what they mean. These include:\n",
    "- \"Relative frequency of each of the stop words mentioned in the comment\"\n",
    "- \"Average number of sentences\"\n",
    "- \"Size of the comments\"\n",
    "\n",
    "What these mean is not unequivocally clear. How should relative frequency be measured? Each comment has a deterministic number of sentences, so the average of sentences over what? The size of the comments, measured in what way? Since such features are ambiguous in their definition and are not reported to be important for the native vs non-native experiment, we exclude them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the problems above, the paper does not detail how models were constructed. It mentions that approximately 322K features were used in the experiment and the baseline is 1/(number of classes). The first number makes sense as we have approximately 323K features in total. However, about 110K of these are native US English, whereas the other 200K are non-native speakers. It is not mentioned whether the non-native comments should be downsampled such that we have a balanced problem. We will assume this is the case.\n",
    "\n",
    "### Repetition of the non-native experiment using SVM classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, we cannot use the cross-validation function from `sklearn`. The reason for this is that the language model distribution has to be reconstructed for each fold so that there is no dependency between training and validation data. Let us therefore do it explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hans/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/hans/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/hans/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/hans/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/hans/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:47: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "/home/hans/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:55: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67011209783083414, 0.67025767942932013, 0.67375163779298297, 0.66792837385354487, 0.67821782178217827]\n"
     ]
    }
   ],
   "source": [
    "# Downsample the non-native languages so that the classes are balanced.\n",
    "randomsample = pd.concat([df[df.native == \"non-native\"].sample(sum(df.native == \"native\")), df[df.native==\"native\"]])\n",
    "randomsample = randomsample.sample(frac=1)\n",
    "randomsample.native = randomsample.native.astype('category')\n",
    "\n",
    "# Parameters to be used for training\n",
    "n = 4    # n-grams for words, PoS tags and word sizes.\n",
    "m = 9    # m-grams for characters\n",
    "k = 5    # Number of folds for cross-validation\n",
    "\n",
    "# Generator for folds. Note data has already been shuffled. Doing it won't hurt anyone though.\n",
    "kf = KFold (n_splits = k, shuffle = True, random_state = k)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for split in kf.split(randomsample):\n",
    "        \n",
    "    # Training and validation data.\n",
    "    training = randomsample.iloc[split[0]]\n",
    "    validation = randomsample.iloc[split[1]]\n",
    "    \n",
    "    # Tokenize sentences and structures for training data\n",
    "    training['tokenized_sents'] = training.apply(lambda row: sent_tokenize(row['text_clean']), axis=1)\n",
    "    training['tokenized_struc'] = training.apply(lambda row: sent_tokenize(row['text_structure']), axis=1)\n",
    "    validation['tokenized_sents'] = validation.apply(lambda row: sent_tokenize(row['text_clean']), axis=1)\n",
    "    validation['tokenized_struc'] = validation.apply(lambda row: sent_tokenize(row['text_structure']), axis=1)\n",
    "    \n",
    "    # Derive the language distribution based on training data.\n",
    "    lang_dis = language_distribution(n,m,2,training[['native','tokenized_sents','tokenized_struc']], training.native.unique())\n",
    "    \n",
    "    # Use the language distribution to obtain features for training data.\n",
    "    features = training.apply(lambda row: compute_all_features(lang_dis,row['tokenized_sents'], row['tokenized_struc']), axis=1)\n",
    "    features = pd.DataFrame(features.to_frame()[0].values.tolist(), index=features.to_frame()[0].index, columns=colnames)\n",
    "    training = pd.merge(training, features, left_index=True, right_index=True)\n",
    "    \n",
    "    # Use the language distribution to compute similarity data for validation data.\n",
    "    features = validation.apply(lambda row: compute_all_features(lang_dis,row['tokenized_sents'], row['tokenized_struc']), axis=1)\n",
    "    features = pd.DataFrame(features.to_frame()[0].values.tolist(), index=features.to_frame()[0].index, columns=colnames)\n",
    "    validation = pd.merge(validation, features, left_index=True, right_index=True)\n",
    "    \n",
    "    # Train linear SVM classifier.\n",
    "    linear = svm.SVC(kernel='linear')\n",
    "    linear.fit(training[colnames], training.native)\n",
    "    y_predicted = linear.predict(validation[colnames])\n",
    "    scores.append(accuracy_score(validation.native, y_predicted))\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems\n",
    "- The language model distribution is based on all training samples. This actually means that our features already know some information about the class labels in cross-validation, which is kind of prohibited. Fixed, but maybe not the most efficient implementation as it has to reconstruct the model k times.\n",
    "- Warnings. Should be able to suppress them. Not a problem for running but ugly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df2 = df.apply(lambda row: compute_all_features(lang_dis,row['tokenized_sents'], row['tokenized_struc']), axis=1)\n",
    "#df2 = pd.DataFrame(df2.to_frame()[0].values.tolist(), index=df2.to_frame()[0].index, columns=colnames)\n",
    "#df = pd.merge(df, df2, left_index=True, right_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
