{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and brief description data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing packages we need and loading the data. We will use pandas to store the data. The scripts intend to follow the same procedure as Al-Rfou, but have been reimplemented from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "from nltk import FreqDist, ngrams, sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold, ParameterGrid\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the training data. Scramble the rows (sometimes this is important for training)\n",
    "df = pd.read_csv(\"python_data/train\",sep=\"\\t\",error_bad_lines=False,encoding=\"utf-8\")\n",
    "df = df.sample(frac=1, random_state = 54021)\n",
    "\n",
    "# Load some datasets we will need later on. E.g. English stopwords.\n",
    "eng_stopwords = stopwords.words('english') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first take a brief look at the data. We see that there are approximately 323K comments in the training data. For each comment, we have the native language of the writer (native_lang), the self-reported level of the english speaker (native, 1, 2, 3, 4, 5 or unknown), the original text of the comment (text_original), the text with demonyms and proper nouns replaced by PoS tags (text_clean), and finally the structure of the text reported by SENNA (text_structure). Of the 323K comments, about 110K comments are from native speakers. Thus, the training data seems sufficiently balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>native_lang</th>\n",
       "      <th>text_original</th>\n",
       "      <th>level_english</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_structure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>323185</td>\n",
       "      <td>323185</td>\n",
       "      <td>323185</td>\n",
       "      <td>323185</td>\n",
       "      <td>323185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>20</td>\n",
       "      <td>317653</td>\n",
       "      <td>7</td>\n",
       "      <td>317560</td>\n",
       "      <td>317281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>EN</td>\n",
       "      <td>is being used on this article. I notice the im...</td>\n",
       "      <td>N</td>\n",
       "      <td>is being used on this article. I notice the im...</td>\n",
       "      <td>VBZ VBG VBN IN DT NN . PRP VBP DT NN NN VBZ IN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>110320</td>\n",
       "      <td>708</td>\n",
       "      <td>110320</td>\n",
       "      <td>708</td>\n",
       "      <td>708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       native_lang                                      text_original  \\\n",
       "count       323185                                             323185   \n",
       "unique          20                                             317653   \n",
       "top             EN  is being used on this article. I notice the im...   \n",
       "freq        110320                                                708   \n",
       "\n",
       "       level_english                                         text_clean  \\\n",
       "count         323185                                             323185   \n",
       "unique             7                                             317560   \n",
       "top                N  is being used on this article. I notice the im...   \n",
       "freq          110320                                                708   \n",
       "\n",
       "                                           text_structure  \n",
       "count                                              323185  \n",
       "unique                                             317281  \n",
       "top     VBZ VBG VBN IN DT NN . PRP VBP DT NN NN VBZ IN...  \n",
       "freq                                                  708  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     323185\n",
       "unique        20\n",
       "top           EN\n",
       "freq      110320\n",
       "Name: native_lang, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.native_lang.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['native'] = np.where(df['native_lang']=='EN', \"native\", \"non-native\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replication Al-'Rfou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper on which we've based our project on uses similarity scores to word and character n-gram models as the features for subsequent classification. Let us embark too on construction of such models. However, other literature has shown that for character n-grams, increasing n seems to enhance classifcation. Thus, we will construct models for up to 10 n-gram models.\n",
    "\n",
    "Note that the two important steps are (i) constructing n-gram models for each language and (ii) computing similarity scores against these distributions as the features. \n",
    "\n",
    "#### (i) `pruned_language_distribution` to construct n-gram models\n",
    "\n",
    "Note, a problem with (i) is that construction of n-grams suffers from combinatorial explosion. This is problematic for our purposes as we have no access to a computer with more than 8 GB RAM. To prevent this combinatorial explosion, we prevent construction of higher order n-grams that do not meet a lower threshold `lowerfreqlimit`. Grams with counts equal to 1 do not contribute to the similarity score. Hence, for a strict replication of Al-'Rfou `lowerfreqlimit` should be set to 1. Where we run into trouble processing data we take the liberty to increase this parameter a little bit.\n",
    "\n",
    "One could argue that increasing this parameter will result in loss of information as it will not record misspellings, which may be indicative of non-native written text. However, note that the character n-grams capture typical non-native speaker mistakes, such that this loss of information is limited. \n",
    "\n",
    "Note that the implementation of pruned_language_distribution may seem not very efficient, as it has to check for each n-gram if the sum of the two (n-1) grams it contains exceeds `lowerfreqlimit`. We provide a little benchmark in the appendix to compare `pruned_language_distribution` againt `language_distribution` where all bigrams are exhaustively constructed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pruned_language_distribution(n, m, lowerfreqlimit, training, LANGUAGES):\n",
    "    \"\"\"Calculate the word n grams distribution up to n, the character n gram distribution up to m.\n",
    "    @n: consider k-grams up to and including n for words, part of speech tags and word sizes.\n",
    "    @m: consider k-grams up to and including m for characters. We assume m >= n.\n",
    "    @lowerfreqlimit: number below which we consider words misspellings, odd words out or unique.\n",
    "    @training: training data to retrieve the language distribution from.\n",
    "    @LANGUAGES: languages based on which we classify.\n",
    "    \"\"\"\n",
    "    \n",
    "    language_dist = {}\n",
    "\n",
    "    for language in LANGUAGES:\n",
    "        language_dist[language] = {\"words\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)])),\n",
    "                               \"tags\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)])),\n",
    "                               \"chars\": dict(zip(range(1, m+1), [FreqDist() for i in range(1, m+1)])),\n",
    "                               \"w_sizes\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)]))}\n",
    "        \n",
    "    # Iterate first over k. This is required as we need to know the full k-1 distributions to see if we should add a \n",
    "    # k-gram to the dictionary.\n",
    "    kmax = 0\n",
    "    for k in range(1, n+1):\n",
    "        for language, tokenized_sents, tokenized_struc in training.itertuples(index=False):\n",
    "            for sentence in tokenized_sents:\n",
    "                \n",
    "                # Get the necessary input structures for the ngrams-function. It is sentence for \"chars\".          \n",
    "                token=word_tokenize(sentence)\n",
    "                wordlens = [len(word) for word in token]\n",
    "                \n",
    "                # Note, for any gram, there exist 2 subgrams of all but the first and all of the last element. Let us\n",
    "                # only update the dictionary if the total count of these subgrams exceeds the lower limit. This prevents\n",
    "                # an unnecessary combinatorial explosion.\n",
    "                for gram in ngrams(sentence,k):\n",
    "                    if k == 1: \n",
    "                        language_dist[language][\"chars\"][k][gram] += 1\n",
    "                    elif language_dist[language][\"chars\"][k-1].get(gram[1:],0)+language_dist[language][\"chars\"][k-1].get(gram[:-1],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"chars\"][k][gram] += 1\n",
    "                        \n",
    "                for gram in ngrams(token,k):\n",
    "                    if k == 1:\n",
    "                        language_dist[language][\"words\"][k][gram] += 1\n",
    "                    elif language_dist[language][\"words\"][k-1].get(gram[1:],0)+language_dist[language][\"words\"][k-1].get(gram[:-1],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"words\"][k][gram] += 1\n",
    "                        \n",
    "                for gram in ngrams(wordlens,k):\n",
    "                    if k == 1:\n",
    "                        language_dist[language][\"w_sizes\"][k][gram] += 1\n",
    "                    elif language_dist[language][\"w_sizes\"][k-1].get(gram[1:],0)+language_dist[language][\"w_sizes\"][k-1].get(gram[:-1],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"w_sizes\"][k][gram] += 1\n",
    "                        \n",
    "            # Now for the tokenized structures (tags)\n",
    "            for sentence in tokenized_struc:\n",
    "                token=word_tokenize(sentence)\n",
    "                for gram in ngrams(token,k):\n",
    "                    if k == 1:\n",
    "                        language_dist[language][\"tags\"][k][gram] += 1\n",
    "                    elif language_dist[language][\"tags\"][k-1].get(gram[1:],0)+language_dist[language][\"tags\"][k-1].get(gram[:-1],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"tags\"][k][gram] += 1\n",
    "                        \n",
    "    # Also construct it for higher order k-grams for characters.\n",
    "    for k in range(n+1, m+1):\n",
    "        for language, tokenized_sents, tokenized_struc in training.itertuples(index=False):\n",
    "            for sentence in tokenized_sents:\n",
    "                for gram in ngrams(sentence,k):\n",
    "                    if language_dist[language][\"chars\"][k-1].get(gram[1:],0)+language_dist[language][\"chars\"][k-1].get(gram[:-1],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"chars\"][k][gram] += 1\n",
    "                           \n",
    "    return language_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the distribution of grams over different languages, we can compute similarity scores. Note, for each language one can compute similarity score against each k-gram model. Thus, this results here in nlang*(3n+m) features. The paper mentiones that the scores are calculated as the sum of the 2-logs of counts in the model. Inspection of the scripts, however, show that this is not the case: this 2-log is normalized by the length of the sequence. We will follow the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_similarity_score(dis_ngramdic, gramlist):\n",
    "    \"\"\" This function computes the similarity scores for a comment based on the corresponding k-grams.\n",
    "    Note that the comment is already tokenized into sentences.\n",
    "    @dis_ngramdic: ngram dictionary as constructed by language_distribution for particular k.\n",
    "    @gramlist: list of kgrams\n",
    "    \"\"\"\n",
    "    score=0\n",
    "    if gramlist:\n",
    "        for gram in gramlist:\n",
    "            score += math.log2(dis_ngramdic.get(gram,1))\n",
    "    return score\n",
    "\n",
    "colnames = None\n",
    "\n",
    "def compute_all_features(lang_dis, tokenized_sent, tokenized_struc):\n",
    "    \"\"\" This function compares the tokenized sentences and tokenized structure to each of the languages distributions.\n",
    "    It returns similarity scores to each language model. Also included are other features, such as the number of sentences\n",
    "    per \n",
    "    @lang_dis: Language distribution of n-grams.\n",
    "    @tokenized_sents: sentences tokenized by nltk.ngrams\n",
    "    @tokenized_struc: PoS structure retrieved by SENNA, tokenized by nltk.ngrams.``\n",
    "    \"\"\"\n",
    "    simscoredict=dict()\n",
    "\n",
    "    # For each gramtype, first construct the list of which we can make n-grams.\n",
    "    wordlens_ps = []\n",
    "    words_ps = []\n",
    "    struc_ps = []\n",
    "    \n",
    "    for sentence in tokenized_sent:\n",
    "        wordlens_ps.append([len(word) for word in word_tokenize(sentence) if word.isalpha()])\n",
    "        words_ps.append([word for word in word_tokenize(sentence)])\n",
    "    \n",
    "    for sentence in tokenized_struc:\n",
    "        struc_ps.append([word for word in word_tokenize(sentence)])\n",
    "    \n",
    "    # Now we should construct k-gram lists for each k and return the score. Let us store all grams in \n",
    "    for gramtype in lang_dis[list(lang_dis.keys())[0]].keys():\n",
    "        \n",
    "        # Select appropriate data type.\n",
    "        if gramtype == \"tags\":\n",
    "            ps = struc_ps\n",
    "        elif gramtype ==\"words\":\n",
    "            ps = words_ps\n",
    "        elif gramtype == \"w_sizes\":\n",
    "            ps = wordlens_ps\n",
    "        else:\n",
    "            ps = tokenized_sent\n",
    "            \n",
    "        seq_len = max(sum([len(item) for item in ps]),1)\n",
    "            \n",
    "        # Construct for each k a gramlist.\n",
    "        for k in range(1,len(lang_dis[list(lang_dis.keys())[0]][gramtype])+1):\n",
    "            \n",
    "            kgramlist = [gram for sentence in ps for gram in ngrams(sentence, k)]\n",
    "            \n",
    "            for lang in lang_dis.keys():\n",
    "                simscoredict[lang+'_'+gramtype+'_'+str(k)]= compute_similarity_score(lang_dis[lang][gramtype][k], kgramlist)/seq_len \n",
    "        \n",
    "    # Set the other features they use in the paper.\n",
    "    simscoredict[\"num_sentences\"] = len(tokenized_sent) if isinstance(tokenized_sent, list) else 0\n",
    "    simscoredict[\"num_words\"] = sum([len(wc) for wc in wordlens_ps])\n",
    "    simscoredict[\"avg_wordlength\"] = sum([sum(word) for word in wordlens_ps])/max(simscoredict[\"num_words\"],1)\n",
    "    \n",
    "    global colnames\n",
    "    if colnames == None:\n",
    "        colnames = list(simscoredict.keys())\n",
    "            \n",
    "    return simscoredict.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al Rfou reports having used some different features too. It is not entirely clear what they mean. These include:\n",
    "- \"Relative frequency of each of the stop words mentioned in the comment\"\n",
    "- \"Average number of sentences\"\n",
    "- \"Size of the comments\"\n",
    "\n",
    "What these mean is not unequivocally clear. How should relative frequency be measured? Each comment has a deterministic number of sentences, so the average of sentences over what? The size of the comments, measured in what way? Since such features are ambiguous in their definition and are not reported to be important for the native vs non-native experiment, we exclude them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the problems above, the paper does not detail how models were constructed. It mentions that approximately 322K features were used in the experiment and the baseline is 1/(number of classes). The first number makes sense as we have approximately 323K features in total. However, about 110K of these are native US English, whereas the other 200K are non-native speakers. It is not mentioned whether the non-native comments should be downsampled such that we have a balanced problem. We will assume this is the case.\n",
    "\n",
    "### Repetition of the non-native experiment using SVM classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the development/validation set is 7 times as small as the training set. Any n-gram expected to be present in the random sample, can be expected to be present 14 times in the training set. We downsample it a bit, but it seems therefore reasonable to require any n-gram in the language distribution to be in there at least 10 times. Note, computing the distribution takes ~1 hour. Try to dump it into a pickle object which may be reloaded later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hans/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:55: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    }
   ],
   "source": [
    "# Downsample the non-native languages so that the classes are balanced. Fix the random_state such that we get the appropriate lang_dis.\n",
    "print(\"Loading the training and validation data...\")\n",
    "training = pd.concat([df[df.native == \"non-native\"].sample(sum(df.native == \"native\"), random_state = 1810), df[df.native==\"native\"]])\n",
    "training = training.sample(frac=1, random_state = 1318910)\n",
    "training.native = training.native.astype('category')\n",
    "\n",
    "# Load the validation data. \n",
    "validation = pd.read_csv(\"python_data/development\",sep=\"\\t\",error_bad_lines=False,encoding=\"utf-8\")\n",
    "df['native'] = np.where(df['native_lang']=='EN', \"native\", \"non-native\")\n",
    "\n",
    "# Parameter choices\n",
    "n = 4           # n-grams for words, PoS tags and word sizes.\n",
    "m = 4           # m-grams for characters\n",
    "lowerlim = 20   # lower limit on the number of wordcounts to consider words for bigrams, trigrams, etc. Needed to prevent memory issues.\n",
    "\n",
    "# Tokenize sentences and structures for training data.\n",
    "print(\"Tokenizing the sentences...\")\n",
    "training['tokenized_sents'] = training.apply(lambda row: sent_tokenize(row['text_clean']), axis=1)\n",
    "training['tokenized_struc'] = training.apply(lambda row: sent_tokenize(row['text_structure']), axis=1)\n",
    "validation['tokenized_sents'] = validation.apply(lambda row: sent_tokenize(row['text_clean']), axis=1)\n",
    "validation['tokenized_struc'] = validation.apply(lambda row: sent_tokenize(row['text_structure']), axis=1)\n",
    "\n",
    "# Drop the original texts as they are no longer needed.\n",
    "training = training.drop(['text_clean', 'text_structure'], axis = 1)\n",
    "validation = validation.drop(['text_clean', 'text_structure'], axis = 1)\n",
    "\n",
    "# Derive the language distribution from the training data.\n",
    "print(\"Deriving the language distribution from training data...\")\n",
    "lang_dis = pruned_language_distribution(n,m,lowerlim,training[['native','tokenized_sents','tokenized_struc']], training.native.unique())\n",
    "\n",
    "# Use the language distribution to obtain features for training data and validation data.\n",
    "print(\"Computing the features for training and validation data.\")\n",
    "features = training.apply(lambda row: compute_all_features(lang_dis,row['tokenized_sents'], row['tokenized_struc']), axis=1)\n",
    "features = pd.DataFrame(features.to_frame()[0].values.tolist(), index=features.to_frame()[0].index, columns=colnames)\n",
    "training = pd.merge(training, features, left_index=True, right_index=True)\n",
    "features = validation.apply(lambda row: compute_all_features(lang_dis,row['tokenized_sents'], row['tokenized_struc']), axis=1)\n",
    "features = pd.DataFrame(features.to_frame()[0].values.tolist(), index=features.to_frame()[0].index, columns=colnames)\n",
    "validation = pd.merge(validation, features, left_index=True, right_index=True)\n",
    "\n",
    "training = training.drop(['tokenized_sents', 'tokenized_struc'], axis=1)\n",
    "validation = validation.drop(['tokenized_sents', 'tokenized_struc'], axis=1)\n",
    "lang_dis.clear()\n",
    "\n",
    "# Drop the tokenized sentences. Also clear the language distribution. They will no longer be needed.\n",
    "training.to_csv(\"python_data/training_features_4_4_\"+str(lowerlim))\n",
    "validation.to_csv(\"python_data/validation_with_features_4_4_\"+str(lowerlim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.728654920313\n"
     ]
    }
   ],
   "source": [
    "# Train the SVC classifier with a linear kernel. This is pursued is in the paper.\n",
    "training=pd.read_csv(\"python_data/training_features_4_4_20\",index_col=0,header=0)\n",
    "validation=pd.read_csv(\"python_data/validation_with_features_4_4_20\",index_col=0,header=0)\n",
    "\n",
    "linear = svm.LinearSVC(C=1.0, penalty=\"l1\", dual=False)\n",
    "linear.fit(training.iloc[:,training.columns.get_loc('non-native_words_1'):], training.native)\n",
    "y_predicted = linear.predict(validation.iloc[:,validation.columns.get_loc('non-native_words_1'):])\n",
    "validation['native'] = np.where(validation['native_lang']=='EN', \"native\", \"non-native\")\n",
    "accur = accuracy_score(validation.native, y_predicted)\n",
    "print(accur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we can achieve a 72.9% accuracy on all the data, which is quite in the league of the 74.53% in the paper. Note that accuracy of course depends on the fold on the training and testing data. For another fold, we found accuracy of 73.0%. Let us investigate if we can bump up this accuracy by parameter tuning through grid search. We will also try to improve this accuracy by using a non-linear kernel. Such a non-linear kernel may make sense as it is not guaranteed at all that the two classes are linearly separable. There is also the consideration of scaling. We did not apply it here as Al-Rfou does not seem to apply it, but it is generally thought to be a good idea for SVMs when one starts working with kernels.\n",
    "\n",
    "#### Tuning C.\n",
    "Seems to barely have any effect. Scores hover around .73. We get a mean score of .73018 and a standard deviation of 0.00034. Standard error on mean is of course even lower. In retrospect, this is not entirely unexpected as increasing the parameter C is a means to increase robustness by lowering the bias of the classifier. Since we have lots of training data, we can expect that it will have little effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.72923720589186736,\n",
       " 0.72906467683150378,\n",
       " 0.7288490155060493,\n",
       " 0.72891371390368564,\n",
       " 0.72897841230132199,\n",
       " 0.72895684616877654,\n",
       " 0.72923720589186736,\n",
       " 0.72865492031314028,\n",
       " 0.72899997843386743,\n",
       " 0.72859022191550393,\n",
       " 0.72919407362677646,\n",
       " 0.72932347042204926,\n",
       " 0.72878431710841296,\n",
       " 0.72921563975932191,\n",
       " 0.72902154456641288,\n",
       " 0.7294313010847765,\n",
       " 0.72932347042204926,\n",
       " 0.72882744937350386,\n",
       " 0.72887058163859475,\n",
       " 0.72934503655459471,\n",
       " 0.72880588324095841]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = [2**i for i in range(-5, 16, 1)]\n",
    "scores = []\n",
    "colnames = list(training)[4:]\n",
    "for C_ in grid:\n",
    "    linear = svm.LinearSVC(C=C_, penalty=\"l1\", dual=False)\n",
    "    linear.fit(training[colnames], training.native)\n",
    "    y_predicted = linear.predict(validation[colnames])\n",
    "    validation['native'] = np.where(validation['native_lang']=='EN', \"native\", \"non-native\")\n",
    "    scores.append(accuracy_score(validation.native, y_predicted))\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging for non-linear kernels & grid-search to optimize parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines are a generalization of Support Vector Classifiers to non-linear decision boundaries. Such non-linear decision boundaries are constructed by a \"kernel trick\". In Hastie, it is reported that when classes are not linearly separable, non-linear kernels may deliver drastic improvements in prediction performance over linear kernels (but at the risk of overfitting as implied by the higher flexibility).\n",
    "\n",
    "Our data are currently classified into two classes, being (i) native English speakers and (ii) non-native speakers. The second group obviously consists of many subgroups, and the separating hyperplanes between each of these subgroups with the native English speakers can be expected to be different. That the classes are linearly separable is therefore inherently not obvious and finding out whether non-linear kernels do better is an interesting problem.\n",
    "\n",
    "However, problem kicks in as the implementation of `svc` in `sklearn` has in the best-case scenario complexity $O(n_features*n_samples^2)$, leading to very large processing times for training SVC. Bagging is a neat way around this problem as a reduction in the number of samples by a factor n_estimators yields a reduction in complexity of n_estimators^2, significantly speeding up calculations (inspired/suggested by https://stackoverflow.com/questions/31681373/making-svm-run-faster-in-python). Let us try this by bagging with 20 estimators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier\n",
      "Prediction out-of-sample\n",
      "Bagging SVC 75.9409556388855 0.729320461859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nstart = timeit.default_timer()\\nsvc = svm.SVC(kernel=\\'rbf\\',gamma=2**-5, C=2**5, cache_size=4000)\\nsvc.fit(training[colnames],training.native)\\ny_predicted=svc.predict(validation[colnames])\\nprint(accuracy_score(validation.native, y_predicted))\\nend = timeit.default_timer()\\nprint(\"Training and prediction completed in {} seconds\".format(end-start))'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Load data and timing package.\n",
    "import time\n",
    "training=pd.read_csv(\"python_data/training_features_4_4_20\",index_col=0,header=0)\n",
    "validation=pd.read_csv(\"python_data/validation_with_features_4_4_20\",index_col=0,header=0)\n",
    "validation['native'] = np.where(validation['native_lang']=='EN', \"native\", \"non-native\")\n",
    "validation = pd.concat([validation[validation.native == \"non-native\"].sample(sum(validation.native == \"native\"), random_state = 1810), validation[validation.native==\"native\"]])\n",
    "\n",
    "colnames = list(training)[4:]\n",
    "\n",
    "# Do scaling. This is suggested by Hsu et al. Scaling is based on training data but executed on validation data.\n",
    "scaler = StandardScaler()\n",
    "training[colnames] = scaler.fit_transform(training[colnames])\n",
    "validation[colnames] = scaler.transform(validation[colnames])\n",
    "\n",
    "# Train SVCs by bagging. 20 estimators will yield approximately 10.000 samples per SVM. This number should be feasible\n",
    "# according to the documentation. We can speed up things by multithreading (n_jobs = -1).\n",
    "n_estimators = 20\n",
    "start = time.time()\n",
    "clf = BaggingClassifier(svm.SVC(kernel='rbf', C=2**5, gamma=2**-5, cache_size=2000), random_state = 1281, max_samples=1.0 / n_estimators, n_jobs = -1, n_estimators=n_estimators)\n",
    "print(\"Fitting the classifier\")\n",
    "clf.fit(training[colnames],training.native)\n",
    "print(\"Prediction out-of-sample\")\n",
    "y_predicted = clf.predict(validation[colnames])\n",
    "end = time.time()\n",
    "print(\"Bagging SVC\", end - start, accuracy_score(validation.native, y_predicted))\n",
    "\n",
    "\n",
    "start = timeit.default_timer()\n",
    "svc = svm.SVC(kernel='rbf',gamma=2**-5, C=2**5, cache_size=4000)\n",
    "svc.fit(training[colnames],training.native)\n",
    "y_predicted=svc.predict(validation[colnames])\n",
    "print(accuracy_score(validation.native, y_predicted))\n",
    "end = timeit.default_timer()\n",
    "print(\"Training and prediction completed in {} seconds\".format(end-start))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With training of the predictor at approximately 90 seconds, grid-search becomes feasible. For this grid search, we follow recommendations by Hsu et al. (\"A practical guide to Support Vector Classification\"). We will refrain from selecting $(C,\\gamma)$ by cross-validation as we have quite a large dataset already. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/99 completed\n",
      "Iteration 10/99 completed\n",
      "Iteration 20/99 completed\n",
      "Iteration 30/99 completed\n",
      "Iteration 40/99 completed\n",
      "Iteration 50/99 completed\n",
      "Iteration 60/99 completed\n",
      "Iteration 70/99 completed\n",
      "Iteration 80/99 completed\n",
      "Iteration 90/99 completed\n",
      "Iteration 99/99 completed\n"
     ]
    }
   ],
   "source": [
    "# Load data and timing package.\n",
    "training=pd.read_csv(\"python_data/training_features_4_4_20\",index_col=0,header=0)\n",
    "validation=pd.read_csv(\"python_data/validation_with_features_4_4_20\",index_col=0,header=0)\n",
    "validation['native'] = np.where(validation['native_lang']=='EN', \"native\", \"non-native\")\n",
    "validation = pd.concat([validation[validation.native == \"non-native\"].sample(sum(validation.native == \"native\"), random_state = 1810), validation[validation.native==\"native\"]])\n",
    "colnames = list(training)[4:]\n",
    "\n",
    "# Scale the data.\n",
    "scaler = StandardScaler()\n",
    "training[colnames] = scaler.fit_transform(training[colnames])\n",
    "validation[colnames] = scaler.transform(validation[colnames])\n",
    "\n",
    "# Define the grid and pre-allocate memory for scores. Set the seed for bagging such that results become comparable.\n",
    "Cs=[2**i for i in range(-5,16,2)]\n",
    "gammas = [2**i for i in range(-15, 3, 2)]\n",
    "param_grid = {'C': Cs, 'gamma': gammas, 'kernel':['rbf'], 'cache_size':[500.0]}\n",
    "scores = pd.DataFrame(columns=['C','gamma','kernel','score'])\n",
    "\n",
    "# Iterate over the grid and save scores for each (C,gamma) pair.\n",
    "itern = 1\n",
    "n_estimators = 20\n",
    "for g in ParameterGrid(param_grid):\n",
    "    svc = svm.SVC()\n",
    "    svc.set_params(**g)\n",
    "    clf = BaggingClassifier(svc, random_state = 1281, max_samples=1.0 / n_estimators, n_jobs = 4, n_estimators=n_estimators)\n",
    "    clf.fit(training[colnames],training.native)\n",
    "    y_predicted = clf.predict(validation[colnames])\n",
    "    g['score'] = [accuracy_score(validation.native, y_predicted)]\n",
    "    holder  = pd.DataFrame(g)\n",
    "    scores = pd.concat([scores, holder])\n",
    "    if (itern==1 or itern%10 == 0 or itern == 99):\n",
    "        print(\"Iteration {}/{} completed\".format(itern, len(Cs)*len(gammas)))\n",
    "    itern += 1\n",
    "        \n",
    "scores.to_csv(\"results_gridsearch_SVM_rbf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>cache_size</th>\n",
       "      <th>gamma</th>\n",
       "      <th>kernel</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.03125</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.03125</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.03125</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.514859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.03125</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.549593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.03125</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.571740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.03125</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.618052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.03125</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.589375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.03125</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.03125</td>\n",
       "      <td>500.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.12500</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.12500</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.519749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.12500</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.553820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.12500</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.601868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.12500</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.700486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.12500</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.710360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.12500</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.674459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.12500</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.571172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.12500</td>\n",
       "      <td>500.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.501325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.517856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.556439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.617705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.723232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.736261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.733643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.715219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.649852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.554073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.557228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.621522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.729825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>512.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.717774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>512.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.685406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>512.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.592309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2048.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.736923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2048.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.732759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2048.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.730898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2048.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.726260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2048.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.720803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2048.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.717332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2048.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.717932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2048.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.685406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2048.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.592309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8192.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.732286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8192.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.729604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8192.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.727333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8192.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.722191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8192.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.717459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8192.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.713988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8192.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.717932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8192.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.685406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8192.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.592309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32768.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.730235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32768.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.727207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32768.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.724399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32768.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.721213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32768.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.714241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32768.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.714178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32768.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.717932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32768.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.685406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32768.00000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.592309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              C  cache_size     gamma kernel     score\n",
       "0       0.03125       500.0  0.000031    rbf  0.500000\n",
       "0       0.03125       500.0  0.000122    rbf  0.500000\n",
       "0       0.03125       500.0  0.000488    rbf  0.514859\n",
       "0       0.03125       500.0  0.001953    rbf  0.549593\n",
       "0       0.03125       500.0  0.007812    rbf  0.571740\n",
       "0       0.03125       500.0  0.031250    rbf  0.618052\n",
       "0       0.03125       500.0  0.125000    rbf  0.589375\n",
       "0       0.03125       500.0  0.500000    rbf  0.500000\n",
       "0       0.03125       500.0  2.000000    rbf  0.500000\n",
       "0       0.12500       500.0  0.000031    rbf  0.500000\n",
       "0       0.12500       500.0  0.000122    rbf  0.519749\n",
       "0       0.12500       500.0  0.000488    rbf  0.553820\n",
       "0       0.12500       500.0  0.001953    rbf  0.601868\n",
       "0       0.12500       500.0  0.007812    rbf  0.700486\n",
       "0       0.12500       500.0  0.031250    rbf  0.710360\n",
       "0       0.12500       500.0  0.125000    rbf  0.674459\n",
       "0       0.12500       500.0  0.500000    rbf  0.571172\n",
       "0       0.12500       500.0  2.000000    rbf  0.501325\n",
       "0       0.50000       500.0  0.000031    rbf  0.517856\n",
       "0       0.50000       500.0  0.000122    rbf  0.556439\n",
       "0       0.50000       500.0  0.000488    rbf  0.617705\n",
       "0       0.50000       500.0  0.001953    rbf  0.723232\n",
       "0       0.50000       500.0  0.007812    rbf  0.736261\n",
       "0       0.50000       500.0  0.031250    rbf  0.733643\n",
       "0       0.50000       500.0  0.125000    rbf  0.715219\n",
       "0       0.50000       500.0  0.500000    rbf  0.649852\n",
       "0       0.50000       500.0  2.000000    rbf  0.554073\n",
       "0       2.00000       500.0  0.000031    rbf  0.557228\n",
       "0       2.00000       500.0  0.000122    rbf  0.621522\n",
       "0       2.00000       500.0  0.000488    rbf  0.729825\n",
       "..          ...         ...       ...    ...       ...\n",
       "0     512.00000       500.0  0.125000    rbf  0.717774\n",
       "0     512.00000       500.0  0.500000    rbf  0.685406\n",
       "0     512.00000       500.0  2.000000    rbf  0.592309\n",
       "0    2048.00000       500.0  0.000031    rbf  0.736923\n",
       "0    2048.00000       500.0  0.000122    rbf  0.732759\n",
       "0    2048.00000       500.0  0.000488    rbf  0.730898\n",
       "0    2048.00000       500.0  0.001953    rbf  0.726260\n",
       "0    2048.00000       500.0  0.007812    rbf  0.720803\n",
       "0    2048.00000       500.0  0.031250    rbf  0.717332\n",
       "0    2048.00000       500.0  0.125000    rbf  0.717932\n",
       "0    2048.00000       500.0  0.500000    rbf  0.685406\n",
       "0    2048.00000       500.0  2.000000    rbf  0.592309\n",
       "0    8192.00000       500.0  0.000031    rbf  0.732286\n",
       "0    8192.00000       500.0  0.000122    rbf  0.729604\n",
       "0    8192.00000       500.0  0.000488    rbf  0.727333\n",
       "0    8192.00000       500.0  0.001953    rbf  0.722191\n",
       "0    8192.00000       500.0  0.007812    rbf  0.717459\n",
       "0    8192.00000       500.0  0.031250    rbf  0.713988\n",
       "0    8192.00000       500.0  0.125000    rbf  0.717932\n",
       "0    8192.00000       500.0  0.500000    rbf  0.685406\n",
       "0    8192.00000       500.0  2.000000    rbf  0.592309\n",
       "0   32768.00000       500.0  0.000031    rbf  0.730235\n",
       "0   32768.00000       500.0  0.000122    rbf  0.727207\n",
       "0   32768.00000       500.0  0.000488    rbf  0.724399\n",
       "0   32768.00000       500.0  0.001953    rbf  0.721213\n",
       "0   32768.00000       500.0  0.007812    rbf  0.714241\n",
       "0   32768.00000       500.0  0.031250    rbf  0.714178\n",
       "0   32768.00000       500.0  0.125000    rbf  0.717932\n",
       "0   32768.00000       500.0  0.500000    rbf  0.685406\n",
       "0   32768.00000       500.0  2.000000    rbf  0.592309\n",
       "\n",
       "[99 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to improve the classification\n",
    "Although it is not obvious, Al Rfou seems to be using the tuning parameter of C=1. Although historically considered a nuisance parameter, it has become clear that C critically determines whether the classifier will overfit. Let us therefore do a grid search for an appropriate C. Cross-validation will not be pursued, since (i) it takes too long and we have ample data and (ii) 5-fold cross-validation performed in a previous version of this script showed the s.e. on MSE is very small (approx. two order of magnitudes smaller than the MSE itself).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems\n",
    "- The language model distribution is based on all training samples. This actually means that our features already know some information about the class labels in cross-validation, which is kind of prohibited. Fixed, but maybe not the most efficient implementation as it has to reconstruct the model k times.\n",
    "- Warnings. Should be able to suppress them. Not a problem for running but ugly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### Benchmark language_distribution against pruned_language_distribution\n",
    "\n",
    "Here we benchmark the function `pruned_language_distribution` against `language_distribution` for a small sample of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def language_distribution(n, m, training, LANGUAGES):\n",
    "    \"\"\"Calculate the word n grams distribution up to n, the character n gram distribution up to m.\n",
    "    @n: consider k-grams up to and including n for words, part of speech tags and word sizes.\n",
    "    @m: consider k-grams up to and including m for characters.\n",
    "    @training: training data to retrieve the language distribution from.\n",
    "    @LANGUAGES: languages based on which we classify.\n",
    "    \"\"\"\n",
    "    \n",
    "    language_dist = {}\n",
    "\n",
    "    for language in LANGUAGES:\n",
    "        language_dist[language] = {\"words\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)])),\n",
    "                               \"tags\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)])),\n",
    "                               \"chars\": dict(zip(range(1, m+1), [FreqDist() for i in range(1, m+1)])),\n",
    "                               \"w_sizes\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)]))}\n",
    "    \n",
    "    for language, tokenized_sents, tokenized_struc in training.itertuples(index=False):\n",
    "    \n",
    "        # Construct n grams counts from the tokenized sentences.\n",
    "        for sentence in tokenized_sents:\n",
    "            token = word_tokenize(sentence)     \n",
    "            wordlens = [len(word) for word in token if word.isalpha()]   \n",
    "            \n",
    "            for k in range(1,n+1):  \n",
    "                language_dist[language][\"w_sizes\"][k].update(ngrams(wordlens,k))\n",
    "                language_dist[language][\"words\"][k].update(ngrams(token,k))\n",
    "                \n",
    "        # Construct n gram counts from sentences tokenized based on structure.\n",
    "        for sentence in tokenized_struc:\n",
    "            token = word_tokenize(sentence)\n",
    "            for k in range(1,n+1):\n",
    "                language_dist[language][\"tags\"][k].update(ngrams(token,k))\n",
    "\n",
    "        # Construct character m-grams for tokenized sentences.\n",
    "        for sentence in tokenized_sents:\n",
    "            for k in range(1,m+1):\n",
    "                language_dist[language][\"chars\"][k].update(ngrams(sentence,k))\n",
    "    \n",
    "    return language_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "def sum_keys(d):\n",
    "    return (0 if not isinstance(d, dict) else len(d) + sum(sum_keys(v) for v in d.values()))\n",
    "\n",
    "rand_sample = df.sample(20000)\n",
    "rand_sample['tokenized_sents'] = rand_sample.apply(lambda row: sent_tokenize(row['text_clean']), axis=1)\n",
    "rand_sample['tokenized_struc'] = rand_sample.apply(lambda row: sent_tokenize(row['text_structure']), axis=1)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "dis1 = language_distribution(4, 9, rand_sample[['native','tokenized_sents','tokenized_struc']], rand_sample.native.unique())\n",
    "end = timeit.default_timer()\n",
    "\n",
    "dis2 = pruned_language_distribution(4, 9, 1, rand_sample[['native','tokenized_sents','tokenized_struc']], rand_sample.native.unique())\n",
    "end2 = timeit.default_timer()\n",
    "\n",
    "print(\"Unpruned, time: {} sec, size: {} items, \\n Pruned, time: {} sec, size: {} items\".format(end-start, sum_keys(dis1), end2-end, sum_keys(dis2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is evident the pruned models are already somewhat better in terms of memory and take only twice as long for construction. If we increase `lowerlimitfreq` this obviously becomes much better. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
