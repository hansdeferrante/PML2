{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and brief description data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing packages we need and loading the data. We will use pandas to store the data. The scripts intend to follow the same procedure as Al-Rfou, but have been reimplemented from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "from nltk import FreqDist, ngrams, sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the training data. Scramble the rows (sometimes this is important for training)\n",
    "df = pd.read_csv(\"python_data/train\",sep=\"\\t\",error_bad_lines=False,encoding=\"utf-8\")\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "# Load some datasets we will need later on. E.g. English stopwords.\n",
    "eng_stopwords = stopwords.words('english') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first take a brief look at the data. We see that there are approximately 323K comments in the training data. For each comment, we have the native language of the writer (native_lang), the self-reported level of the english speaker (native, 1, 2, 3, 4, 5 or unknown), the original text of the comment (text_original), the text with demonyms and proper nouns replaced by PoS tags (text_clean), and finally the structure of the text reported by SENNA (text_structure). Of the 323K comments, about 110K comments are from native speakers. Thus, the training data seems sufficiently balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>native_lang</th>\n",
       "      <th>text_original</th>\n",
       "      <th>level_english</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_structure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>323185</td>\n",
       "      <td>323185</td>\n",
       "      <td>323185</td>\n",
       "      <td>323185</td>\n",
       "      <td>323185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>20</td>\n",
       "      <td>317653</td>\n",
       "      <td>7</td>\n",
       "      <td>317560</td>\n",
       "      <td>317281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>EN</td>\n",
       "      <td>is being used on this article. I notice the im...</td>\n",
       "      <td>N</td>\n",
       "      <td>is being used on this article. I notice the im...</td>\n",
       "      <td>VBZ VBG VBN IN DT NN . PRP VBP DT NN NN VBZ IN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>110320</td>\n",
       "      <td>708</td>\n",
       "      <td>110320</td>\n",
       "      <td>708</td>\n",
       "      <td>708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       native_lang                                      text_original  \\\n",
       "count       323185                                             323185   \n",
       "unique          20                                             317653   \n",
       "top             EN  is being used on this article. I notice the im...   \n",
       "freq        110320                                                708   \n",
       "\n",
       "       level_english                                         text_clean  \\\n",
       "count         323185                                             323185   \n",
       "unique             7                                             317560   \n",
       "top                N  is being used on this article. I notice the im...   \n",
       "freq          110320                                                708   \n",
       "\n",
       "                                           text_structure  \n",
       "count                                              323185  \n",
       "unique                                             317281  \n",
       "top     VBZ VBG VBN IN DT NN . PRP VBP DT NN NN VBZ IN...  \n",
       "freq                                                  708  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     323185\n",
       "unique        20\n",
       "top           EN\n",
       "freq      110320\n",
       "Name: native_lang, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.native_lang.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>native_lang</th>\n",
       "      <th>text_original</th>\n",
       "      <th>level_english</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_structure</th>\n",
       "      <th>native</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>323185</td>\n",
       "      <td>323185</td>\n",
       "      <td>323185</td>\n",
       "      <td>323185</td>\n",
       "      <td>323185</td>\n",
       "      <td>323185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>20</td>\n",
       "      <td>317653</td>\n",
       "      <td>7</td>\n",
       "      <td>317560</td>\n",
       "      <td>317281</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>EN</td>\n",
       "      <td>is being used on this article. I notice the im...</td>\n",
       "      <td>N</td>\n",
       "      <td>is being used on this article. I notice the im...</td>\n",
       "      <td>VBZ VBG VBN IN DT NN . PRP VBP DT NN NN VBZ IN...</td>\n",
       "      <td>non-native</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>110320</td>\n",
       "      <td>708</td>\n",
       "      <td>110320</td>\n",
       "      <td>708</td>\n",
       "      <td>708</td>\n",
       "      <td>212865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       native_lang                                      text_original  \\\n",
       "count       323185                                             323185   \n",
       "unique          20                                             317653   \n",
       "top             EN  is being used on this article. I notice the im...   \n",
       "freq        110320                                                708   \n",
       "\n",
       "       level_english                                         text_clean  \\\n",
       "count         323185                                             323185   \n",
       "unique             7                                             317560   \n",
       "top                N  is being used on this article. I notice the im...   \n",
       "freq          110320                                                708   \n",
       "\n",
       "                                           text_structure      native  \n",
       "count                                              323185      323185  \n",
       "unique                                             317281           2  \n",
       "top     VBZ VBG VBN IN DT NN . PRP VBP DT NN NN VBZ IN...  non-native  \n",
       "freq                                                  708      212865  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['native'] = np.where(df['native_lang']=='EN', \"native\", \"non-native\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replication Al-'Rfou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper on which we've based our project on uses similarity scores to word and character n-gram models as the features for subsequent classification. Let us embark too on construction of such models. However, other literature has shown that for character n-grams, increasing n seems to enhance classifcation. Thus, we will construct models for up to 10 n-gram models.\n",
    "\n",
    "Note that the two important steps are (i) constructing n-gram models for each language and (ii) computing similarity scores against these distributions as the features. \n",
    "\n",
    "#### (i) `pruned_language_distribution` to construct n-gram models\n",
    "\n",
    "Note, a problem with (i) is that construction of n-grams suffers from combinatorial explosion. This is problematic for our purposes as we have no access to a computer with more than 8 GB RAM. To prevent this combinatorial explosion, we prevent construction of higher order n-grams that do not meet a lower threshold `lowerfreqlimit`. Grams with counts equal to 1 do not contribute to the similarity score. Hence, for a strict replication of Al-'Rfou `lowerfreqlimit` should be set to 1. Where we run into trouble processing data we take the liberty to increase this parameter a little bit.\n",
    "\n",
    "One could argue that increasing this parameter will result in loss of information as it will not record misspellings, which may be indicative of non-native written text. However, note that the character n-grams capture typical non-native speaker mistakes, such that this loss of information is limited. \n",
    "\n",
    "Note that the implementation of pruned_language_distribution may seem not very efficient, as it has to check for each n-gram if the sum of the two (n-1) grams it contains exceeds `lowerfreqlimit`. We provide a little benchmark in the appendix to compare `pruned_language_distribution` againt `language_distribution` where all bigrams are exhaustively constructed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pruned_language_distribution(n, m, lowerfreqlimit, training, LANGUAGES):\n",
    "    \"\"\"Calculate the word n grams distribution up to n, the character n gram distribution up to m.\n",
    "    @n: consider k-grams up to and including n for words, part of speech tags and word sizes.\n",
    "    @m: consider k-grams up to and including m for characters. We assume m >= n.\n",
    "    @lowerfreqlimit: number below which we consider words misspellings, odd words out or unique.\n",
    "    @training: training data to retrieve the language distribution from.\n",
    "    @LANGUAGES: languages based on which we classify.\n",
    "    \"\"\"\n",
    "    \n",
    "    language_dist = {}\n",
    "\n",
    "    for language in LANGUAGES:\n",
    "        language_dist[language] = {\"words\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)])),\n",
    "                               \"tags\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)])),\n",
    "                               \"chars\": dict(zip(range(1, m+1), [FreqDist() for i in range(1, m+1)])),\n",
    "                               \"w_sizes\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)]))}\n",
    "        \n",
    "    # Iterate first over k. This is required as we need to know the full k-1 distributions to see if we should add a \n",
    "    # k-gram to the dictionary.\n",
    "    kmax = 0\n",
    "    for k in range(1, n+1):\n",
    "        for language, tokenized_sents, tokenized_struc in training.itertuples(index=False):\n",
    "            for sentence in tokenized_sents:\n",
    "                \n",
    "                # Get the necessary input structures for the ngrams-function. It is sentence for \"chars\".          \n",
    "                token=word_tokenize(sentence)\n",
    "                wordlens = [len(word) for word in token]\n",
    "                \n",
    "                # Note, for any gram, there exist 2 subgrams of all but the first and all of the last element. Let us\n",
    "                # only update the dictionary if the total count of these subgrams exceeds the lower limit. This prevents\n",
    "                # an unnecessary combinatorial explosion.\n",
    "                for gram in ngrams(sentence,k):\n",
    "                    if k == 1: \n",
    "                        language_dist[language][\"chars\"][k].update(gram)\n",
    "                    elif language_dist[language][\"chars\"][k-1].get(gram[0] if k == 2 else gram[:-1],0)+language_dist[language][\"chars\"][k-1].get(gram[1] if k == 2 else gram[1:],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"chars\"][k][gram] += 1\n",
    "                        \n",
    "                for gram in ngrams(token,k):\n",
    "                    if k == 1:\n",
    "                        language_dist[language][\"words\"][k].update(gram)\n",
    "                    elif language_dist[language][\"words\"][k-1].get(gram[0] if k == 2 else gram[:-1],0)+language_dist[language][\"words\"][k-1].get(gram[1] if k == 2 else gram[1:],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"words\"][k][gram] += 1\n",
    "                        \n",
    "                for gram in ngrams(wordlens,k):\n",
    "                    if k == 1:\n",
    "                        language_dist[language][\"w_sizes\"][k].update(gram)\n",
    "                    elif language_dist[language][\"w_sizes\"][k-1].get(gram[0] if k == 2 else gram[:-1],0)+language_dist[language][\"w_sizes\"][k-1].get(gram[1] if k == 2 else gram[1:],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"w_sizes\"][k][gram] += 1\n",
    "                        \n",
    "            # Now for the tokenized structures (tags)\n",
    "            for sentence in tokenized_struc:\n",
    "                token=word_tokenize(sentence)\n",
    "                for gram in ngrams(token,k):\n",
    "                    if k == 1:\n",
    "                        language_dist[language][\"tags\"][k].update(gram)\n",
    "                    elif language_dist[language][\"tags\"][k-1].get(gram[0] if k == 2 else gram[:-1],0)+language_dist[language][\"tags\"][k-1].get(gram[1] if k == 2 else gram[1:],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"tags\"][k][gram] += 1\n",
    "                        \n",
    "    # Also construct it for higher order k-grams for characters.\n",
    "    for k in range(n+1, m+1):\n",
    "        for language, tokenized_sents, tokenized_struc in training.itertuples(index=False):\n",
    "            for sentence in tokenized_sents:\n",
    "                for gram in ngrams(sentence,k):\n",
    "                    if language_dist[language][\"chars\"][k-1].get(gram[0] if k == 2 else gram[:-1],0)+language_dist[language][\"chars\"][k-1].get(gram[0] if k == 2 else gram[:-1],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"chars\"][k][gram] += 1\n",
    "                           \n",
    "    return language_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the distribution of grams over different languages, we can compute similarity scores. Note, for each language one can compute similarity score against each k-gram model. Thus, this results here in nlang*(3n+m) features. The paper mentiones that the scores are calculated as the sum of the 2-logs of counts in the model. Inspection of the scripts, however, show that this is not the case: this 2-log is normalized by the length of the sequence. We will follow the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_similarity_score(dis_ngramdic, gramlist):\n",
    "    \"\"\" This function computes the similarity scores for a comment based on the corresponding k-grams.\n",
    "    Note that the comment is already tokenized into sentences.\n",
    "    @dis_ngramdic: ngram dictionary as constructed by language_distribution for particular k.\n",
    "    @gramlist: list of kgrams\n",
    "    \"\"\"\n",
    "    score=0\n",
    "    if gramlist:\n",
    "        for gram in gramlist:\n",
    "            score += math.log2(dis_ngramdic.get(gram,1))\n",
    "    return score\n",
    "\n",
    "colnames = None\n",
    "\n",
    "def compute_all_features(lang_dis, tokenized_sent, tokenized_struc):\n",
    "    \"\"\" This function compares the tokenized sentences and tokenized structure to each of the languages distributions.\n",
    "    It returns similarity scores to each language model. Also included are other features, such as the number of sentences\n",
    "    per \n",
    "    @lang_dis: Language distribution of n-grams.\n",
    "    @tokenized_sents: sentences tokenized by nltk.ngrams\n",
    "    @tokenized_struc: PoS structure retrieved by SENNA, tokenized by nltk.ngrams.``\n",
    "    \"\"\"\n",
    "    simscoredict=dict()\n",
    "\n",
    "    # For each gramtype, first construct the list of which we can make n-grams.\n",
    "    wordlens_ps = []\n",
    "    words_ps = []\n",
    "    struc_ps = []\n",
    "    \n",
    "    for sentence in tokenized_sent:\n",
    "        wordlens_ps.append([len(word) for word in word_tokenize(sentence) if word.isalpha()])\n",
    "        words_ps.append([word for word in word_tokenize(sentence)])\n",
    "    \n",
    "    for sentence in tokenized_struc:\n",
    "        struc_ps.append([word for word in word_tokenize(sentence)])\n",
    "    \n",
    "    # Now we should construct k-gram lists for each k and return the score. Let us store all grams in \n",
    "    for gramtype in lang_dis[list(lang_dis.keys())[0]].keys():\n",
    "        \n",
    "        # Select appropriate data type.\n",
    "        if gramtype == \"tags\":\n",
    "            ps = struc_ps\n",
    "        elif gramtype ==\"words\":\n",
    "            ps = words_ps\n",
    "        elif gramtype == \"w_sizes\":\n",
    "            ps = wordlens_ps\n",
    "        else:\n",
    "            ps = tokenized_struc\n",
    "            \n",
    "        seq_len = sum([len(item) for item in ps])\n",
    "            \n",
    "        # Construct for each k a gramlist. \n",
    "        for k in range(1,len(lang_dis[list(lang_dis.keys())[0]][gramtype])+1):\n",
    "            \n",
    "            kgramlist = list(set([gram for sentence in ps for gram in ngrams(sentence, k)]))\n",
    "            \n",
    "            for lang in lang_dis.keys():\n",
    "                simscoredict[lang+'_'+gramtype+'_'+str(k)]= compute_similarity_score(lang_dis[lang][gramtype][k], kgramlist)/seq_len \n",
    "        \n",
    "    # Set the other features they use in the paper.\n",
    "    simscoredict[\"num_sentences\"] = len(tokenized_sent) if isinstance(tokenized_sent, list) else 0\n",
    "    simscoredict[\"num_words\"] = sum([len(wc) for wc in wordlens_ps])\n",
    "    simscoredict[\"avg_wordlength\"] = sum([sum(word) for word in wordlens_ps])/simscoredict[\"num_words\"]\n",
    "    \n",
    "    global colnames\n",
    "    if colnames == None:\n",
    "        colnames = list(simscoredict.keys())\n",
    "            \n",
    "    return simscoredict.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al Rfou reports having used some different features too. It is not entirely clear what they mean. These include:\n",
    "- \"Relative frequency of each of the stop words mentioned in the comment\"\n",
    "- \"Average number of sentences\"\n",
    "- \"Size of the comments\"\n",
    "\n",
    "What these mean is not unequivocally clear. How should relative frequency be measured? Each comment has a deterministic number of sentences, so the average of sentences over what? The size of the comments, measured in what way? Since such features are ambiguous in their definition and are not reported to be important for the native vs non-native experiment, we exclude them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the problems above, the paper does not detail how models were constructed. It mentions that approximately 322K features were used in the experiment and the baseline is 1/(number of classes). The first number makes sense as we have approximately 323K features in total. However, about 110K of these are native US English, whereas the other 200K are non-native speakers. It is not mentioned whether the non-native comments should be downsampled such that we have a balanced problem. We will assume this is the case.\n",
    "\n",
    "### Repetition of the non-native experiment using SVM classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, we cannot use the cross-validation function from `sklearn`. The reason for this is that the language model distribution has to be reconstructed for each fold so that there is no dependency between training and validation data. Let us therefore do it explicitly.\n",
    "\n",
    "For now, we will also not pursue using `lowerlim` = 1, even though the paper seemingly uses this. We will set it to 3*k here such that we can expect a randomly drawn gram from the model to also be in the validation chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hans/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/hans/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/hans/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/hans/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/hans/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:32: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "/home/hans/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "/home/hans/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "/home/hans/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:53: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "/home/hans/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:55: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    }
   ],
   "source": [
    "# Downsample the non-native languages so that the classes are balanced.\n",
    "randomsample = pd.concat([df[df.native == \"non-native\"].sample(sum(df.native == \"native\")), df[df.native==\"native\"]])\n",
    "randomsample = randomsample.sample(frac=1)\n",
    "randomsample.native = randomsample.native.astype('category')\n",
    "\n",
    "# Parameters to be used for training\n",
    "n = 4           # n-grams for words, PoS tags and word sizes.\n",
    "m = 4           # m-grams for characters\n",
    "k = 5           # Number of folds for cross-validation\n",
    "lowerlim = 3*k  # lower limit on the number of wordcounts to consider words for bigrams, trigrams, etc. Needed to prevent memory issues.\n",
    "\n",
    "# Generator for folds. Note data has already been shuffled. Doing it won't hurt anyone though.\n",
    "kf = KFold (n_splits = k, shuffle = True, random_state = k)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for split in kf.split(randomsample):\n",
    "        \n",
    "    # Training and validation data.\n",
    "    training = randomsample.iloc[split[0]]\n",
    "    validation = randomsample.iloc[split[1]]\n",
    "    \n",
    "    # Tokenize sentences and structures for training data\n",
    "    training['tokenized_sents'] = training.apply(lambda row: sent_tokenize(row['text_clean']), axis=1)\n",
    "    training['tokenized_struc'] = training.apply(lambda row: sent_tokenize(row['text_structure']), axis=1)\n",
    "    validation['tokenized_sents'] = validation.apply(lambda row: sent_tokenize(row['text_clean']), axis=1)\n",
    "    validation['tokenized_struc'] = validation.apply(lambda row: sent_tokenize(row['text_structure']), axis=1)\n",
    "    \n",
    "    # Derive the language distribution based on training data.\n",
    "    lang_dis = pruned_language_distribution(n,m,lowerlim,training[['native','tokenized_sents','tokenized_struc']], training.native.unique())\n",
    "    \n",
    "    # Use the language distribution to obtain features for training data.\n",
    "    features = training.apply(lambda row: compute_all_features(lang_dis,row['tokenized_sents'], row['tokenized_struc']), axis=1)\n",
    "    features = pd.DataFrame(features.to_frame()[0].values.tolist(), index=features.to_frame()[0].index, columns=colnames)\n",
    "    training = pd.merge(training, features, left_index=True, right_index=True)\n",
    "    \n",
    "    # Use the language distribution to compute similarity data for validation data.\n",
    "    features = validation.apply(lambda row: compute_all_features(lang_dis,row['tokenized_sents'], row['tokenized_struc']), axis=1)\n",
    "    features = pd.DataFrame(features.to_frame()[0].values.tolist(), index=features.to_frame()[0].index, columns=colnames)\n",
    "    validation = pd.merge(validation, features, left_index=True, right_index=True)\n",
    "    \n",
    "    # After we constructed the features, the dictionary is no longer necessary. Clear it.\n",
    "    lang_dis.clear()\n",
    "    \n",
    "    # Train linear SVM classifier.\n",
    "    linear = svm.SVC(kernel='linear')\n",
    "    linear.fit(training[colnames], training.native)\n",
    "    y_predicted = linear.predict(validation[colnames])\n",
    "    scores.append(accuracy_score(validation.native, y_predicted))\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems\n",
    "- The language model distribution is based on all training samples. This actually means that our features already know some information about the class labels in cross-validation, which is kind of prohibited. Fixed, but maybe not the most efficient implementation as it has to reconstruct the model k times.\n",
    "- Warnings. Should be able to suppress them. Not a problem for running but ugly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### Benchmark language_distribution against pruned_language_distribution\n",
    "\n",
    "Here we benchmark the function `pruned_language_distribution` against `language_distribution` for a small sample of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def language_distribution(n, m, training, LANGUAGES):\n",
    "    \"\"\"Calculate the word n grams distribution up to n, the character n gram distribution up to m.\n",
    "    @n: consider k-grams up to and including n for words, part of speech tags and word sizes.\n",
    "    @m: consider k-grams up to and including m for characters.\n",
    "    @training: training data to retrieve the language distribution from.\n",
    "    @LANGUAGES: languages based on which we classify.\n",
    "    \"\"\"\n",
    "    \n",
    "    language_dist = {}\n",
    "\n",
    "    for language in LANGUAGES:\n",
    "        language_dist[language] = {\"words\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)])),\n",
    "                               \"tags\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)])),\n",
    "                               \"chars\": dict(zip(range(1, m+1), [FreqDist() for i in range(1, m+1)])),\n",
    "                               \"w_sizes\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)]))}\n",
    "    \n",
    "    for language, tokenized_sents, tokenized_struc in training.itertuples(index=False):\n",
    "    \n",
    "        # Construct n grams counts from the tokenized sentences.\n",
    "        for sentence in tokenized_sents:\n",
    "            token = word_tokenize(sentence)     \n",
    "            wordlens = [len(word) for word in token if word.isalpha()]   \n",
    "            \n",
    "            for k in range(1,n+1):  \n",
    "                language_dist[language][\"w_sizes\"][k].update(ngrams(wordlens,k))\n",
    "                language_dist[language][\"words\"][k].update(ngrams(token,k))\n",
    "                \n",
    "        # Construct n gram counts from sentences tokenized based on structure.\n",
    "        for sentence in tokenized_struc:\n",
    "            token = word_tokenize(sentence)\n",
    "            for k in range(1,n+1):\n",
    "                language_dist[language][\"tags\"][k].update(ngrams(token,k))\n",
    "\n",
    "        # Construct character m-grams for tokenized sentences.\n",
    "        for sentence in tokenized_sents:\n",
    "            for k in range(1,m+1):\n",
    "                language_dist[language][\"chars\"][k].update(ngrams(sentence,k))\n",
    "    \n",
    "    return language_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "def sum_keys(d):\n",
    "    return (0 if not isinstance(d, dict) else len(d) + sum(sum_keys(v) for v in d.values()))\n",
    "\n",
    "rand_sample = df.sample(20000)\n",
    "rand_sample['tokenized_sents'] = rand_sample.apply(lambda row: sent_tokenize(row['text_clean']), axis=1)\n",
    "rand_sample['tokenized_struc'] = rand_sample.apply(lambda row: sent_tokenize(row['text_structure']), axis=1)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "dis1 = language_distribution(4, 9, rand_sample[['native','tokenized_sents','tokenized_struc']], rand_sample.native.unique())\n",
    "end = timeit.default_timer()\n",
    "\n",
    "dis2 = pruned_language_distribution(4, 9, 1, rand_sample[['native','tokenized_sents','tokenized_struc']], rand_sample.native.unique())\n",
    "end2 = timeit.default_timer()\n",
    "\n",
    "print(\"Unpruned, time: {} sec, size: {} items, \\n Pruned, time: {} sec, size: {} items\".format(end-start, sum_keys(dis1), end2-end, sum_keys(dis2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is evident the pruned models are already somewhat better in terms of memory and take only twice as long for construction. If we increase `lowerlimitfreq` this obviously becomes much better. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
