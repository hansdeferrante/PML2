{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and brief description data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing packages we need and loading the data. We will use pandas to store the data. The scripts intend to follow the same procedure as Al-Rfou, but have been reimplemented from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "from nltk import FreqDist, ngrams, sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold, ParameterGrid, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import pickle\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        x = pickle.load(f)\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the training and validation data...\n"
     ]
    }
   ],
   "source": [
    "# Load the training data. Scramble the rows (sometimes this is important for training)\n",
    "df = pd.read_csv(\"python_data/train\",sep=\"\\t\",error_bad_lines=False,encoding=\"utf-8\")\n",
    "df = df.sample(frac=1, random_state = 54021)\n",
    "df['native'] = np.where(df['native_lang']=='EN', \"native\", \"non-native\")\n",
    "\n",
    "# Load some datasets we will need later on. E.g. English stopwords.\n",
    "eng_stopwords = stopwords.words('english') \n",
    "\n",
    "# Load the training data. Downsample non-English such that it is balanced.\n",
    "print(\"Loading the training and validation data...\")\n",
    "training = pd.concat([df[df.native == \"non-native\"].sample(sum(df.native == \"native\"), random_state = 1810), df[df.native==\"native\"]])\n",
    "training = training.sample(frac=1, random_state = 1318910)\n",
    "training.native = training.native.astype('category')\n",
    "\n",
    "# Load the validation data. Again, downsample such that it is balanced.\n",
    "validation = pd.read_csv(\"python_data/development\",sep=\"\\t\",error_bad_lines=False,encoding=\"utf-8\")\n",
    "validation['native'] = np.where(validation['native_lang']=='EN', \"native\", \"non-native\")\n",
    "validation = pd.concat([validation[validation.native == \"non-native\"].sample(sum(validation.native == \"native\"), random_state = 1), validation[validation.native==\"native\"]])\n",
    "validation.native = validation.native.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first take a brief look at the data. We see that there are approximately 323K comments in the training data. For each comment, we have the native language of the writer (native_lang), the self-reported level of the english speaker (native, 1, 2, 3, 4, 5 or unknown), the original text of the comment (text_original), the text with demonyms and proper nouns replaced by PoS tags (text_clean), and finally the structure of the text reported by SENNA (text_structure). Of the 323K comments, about 110K comments are from native speakers. We downsample to get balanced training and validation data as this is suggested by the paper as its baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>native_lang</th>\n",
       "      <th>text_original</th>\n",
       "      <th>level_english</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_structure</th>\n",
       "      <th>native</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>323185</td>\n",
       "      <td>323185</td>\n",
       "      <td>323185</td>\n",
       "      <td>323185</td>\n",
       "      <td>323185</td>\n",
       "      <td>323185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>20</td>\n",
       "      <td>317653</td>\n",
       "      <td>7</td>\n",
       "      <td>317560</td>\n",
       "      <td>317281</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>EN</td>\n",
       "      <td>is being used on this article. I notice the im...</td>\n",
       "      <td>N</td>\n",
       "      <td>is being used on this article. I notice the im...</td>\n",
       "      <td>VBZ VBG VBN IN DT NN . PRP VBP DT NN NN VBZ IN...</td>\n",
       "      <td>non-native</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>110320</td>\n",
       "      <td>708</td>\n",
       "      <td>110320</td>\n",
       "      <td>708</td>\n",
       "      <td>708</td>\n",
       "      <td>212865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       native_lang                                      text_original  \\\n",
       "count       323185                                             323185   \n",
       "unique          20                                             317653   \n",
       "top             EN  is being used on this article. I notice the im...   \n",
       "freq        110320                                                708   \n",
       "\n",
       "       level_english                                         text_clean  \\\n",
       "count         323185                                             323185   \n",
       "unique             7                                             317560   \n",
       "top                N  is being used on this article. I notice the im...   \n",
       "freq          110320                                                708   \n",
       "\n",
       "                                           text_structure      native  \n",
       "count                                              323185      323185  \n",
       "unique                                             317281           2  \n",
       "top     VBZ VBG VBN IN DT NN . PRP VBP DT NN NN VBZ IN...  non-native  \n",
       "freq                                                  708      212865  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     323185\n",
       "unique        20\n",
       "top           EN\n",
       "freq      110320\n",
       "Name: native_lang, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.native_lang.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop the original dataframe. We will work only with downsampled training and validation data.\n",
    "df = df.iloc[0:0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replication Al-'Rfou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper on which we've based our project on uses similarity scores to word and character n-gram models as the features for subsequent classification. Here, we embark too on construction of such models. The two important steps are (i) constructing n-gram models for each language and (ii) computing similarity scores against these distributions as the features.\n",
    "\n",
    "#### (i) `pruned_language_distribution` to construct n-gram models\n",
    "\n",
    "Note, a problem with (i) is that construction of n-grams suffers from combinatorial explosion. This is problematic for our purposes as we have no access to a computer with more than 8 GB RAM. To prevent this combinatorial explosion, we prevent construction of higher order n-grams that do not meet a lower threshold `lowerfreqlimit`. Grams with counts equal to 1 do not contribute to the similarity score. Hence, for a strict replication of Al-'Rfou `lowerfreqlimit` should be set to 1. We have decided to increase this parameter to 10 to so as to keep relatively sparse n-gram models.\n",
    "\n",
    "One could argue that increasing this parameter will result in loss of information as it will not record misspellings, which may be indicative of non-native written text. However, note that the character n-grams capture typical non-native speaker mistakes, such that this loss of information is limited. \n",
    "\n",
    "Note that the implementation of pruned_language_distribution may seem not very efficient, as it has to check for each n-gram if the sum of the two (n-1) grams it contains exceeds `lowerfreqlimit`. We provide a little benchmark in the appendix to compare `pruned_language_distribution` against `language_distribution` where all bigrams are exhaustively constructed. This is not pursued for the entire training set as it results in a memory error on a PC with 8GB RAM, but it shows that (for smaller samples) preventing construction of leaves with $<10$ counts slows the construction down only by a factor of approximately 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pruned_language_distribution(n, m, lowerfreqlimit, training, LANGUAGES):\n",
    "    \"\"\"Calculate the word n grams distribution up to n, the character n gram distribution up to m.\n",
    "    @n: consider k-grams up to and including n for words, part of speech tags and word sizes.\n",
    "    @m: consider k-grams up to and including m for characters. We assume m >= n.\n",
    "    @lowerfreqlimit: number below which we consider words misspellings, odd words out or unique.\n",
    "    @training: training data to retrieve the language distribution from.\n",
    "    @LANGUAGES: languages based on which we classify.\n",
    "    \"\"\"\n",
    "    \n",
    "    language_dist = {}\n",
    "\n",
    "    for language in LANGUAGES:\n",
    "        language_dist[language] = {\"words\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)])),\n",
    "                               \"tags\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)])),\n",
    "                               \"chars\": dict(zip(range(1, m+1), [FreqDist() for i in range(1, m+1)])),\n",
    "                               \"w_sizes\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)]))}\n",
    "        \n",
    "    # Iterate first over k. This is required as we need to know the full k-1 distributions to see if we should add a \n",
    "    # k-gram to the dictionary.\n",
    "    kmax = 0\n",
    "    for k in range(1, n+1):\n",
    "        for language, text, struc in training.itertuples(index=False):\n",
    "            \n",
    "            for sentence in sent_tokenize(text):\n",
    "                \n",
    "                # Get the necessary input structures for the ngrams-function. It is sentence for \"chars\".\n",
    "                token=word_tokenize(sentence) \n",
    "                wordlens = [len(word) for word in token]\n",
    "                \n",
    "                # Note, for any gram, there exist 2 subgrams of all but the first and all of the last element. Let us\n",
    "                # only update the dictionary if the total count of these subgrams exceeds the lower limit. This prevents\n",
    "                # an unnecessary combinatorial explosion.\n",
    "                for gram in ngrams(sentence,k):\n",
    "                    if k == 1: \n",
    "                        language_dist[language][\"chars\"][k][gram] += 1\n",
    "                    elif language_dist[language][\"chars\"][k-1].get(gram[1:],0)+language_dist[language][\"chars\"][k-1].get(gram[:-1],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"chars\"][k][gram] += 1\n",
    "                        \n",
    "                for gram in ngrams(token,k):\n",
    "                    if k == 1:\n",
    "                        language_dist[language][\"words\"][k][gram] += 1\n",
    "                    elif language_dist[language][\"words\"][k-1].get(gram[1:],0)+language_dist[language][\"words\"][k-1].get(gram[:-1],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"words\"][k][gram] += 1\n",
    "                        \n",
    "                for gram in ngrams(wordlens,k):\n",
    "                    if k == 1:\n",
    "                        language_dist[language][\"w_sizes\"][k][gram] += 1\n",
    "                    elif language_dist[language][\"w_sizes\"][k-1].get(gram[1:],0)+language_dist[language][\"w_sizes\"][k-1].get(gram[:-1],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"w_sizes\"][k][gram] += 1\n",
    "                        \n",
    "            # Now for the tokenized structures (tags)\n",
    "            for sentence in sent_tokenize(struc):\n",
    "                token=word_tokenize(sentence)\n",
    "                for gram in ngrams(token,k):\n",
    "                    if k == 1:\n",
    "                        language_dist[language][\"tags\"][k][gram] += 1\n",
    "                    elif language_dist[language][\"tags\"][k-1].get(gram[1:],0)+language_dist[language][\"tags\"][k-1].get(gram[:-1],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"tags\"][k][gram] += 1\n",
    "                        \n",
    "    # Also construct it for higher order k-grams for characters.\n",
    "    for k in range(n+1, m+1):\n",
    "        for language, tokenized_sents, tokenized_struc in training.itertuples(index=False):\n",
    "            for sentence in tokenized_sents:\n",
    "                for gram in ngrams(sentence,k):\n",
    "                    if language_dist[language][\"chars\"][k-1].get(gram[1:],0)+language_dist[language][\"chars\"][k-1].get(gram[:-1],0) > 2*lowerfreqlimit:\n",
    "                        language_dist[language][\"chars\"][k][gram] += 1\n",
    "                           \n",
    "    return language_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the distribution of grams over different languages, we can compute similarity scores. Note, for each language one can compute similarity score against each k-gram model. Thus, this results here in nlang*(3n+m) features. The paper mentiones that the scores are calculated as the sum of the 2-logs of counts in the model. Inspection of the scripts, however, show that this is not the case: this 2-log is divided by the length of the sequence. We follow the script as we obtain values of infinity if we do not apply this division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_similarity_score(dis_ngramdic, gramlist):\n",
    "    \"\"\" This function computes the similarity scores for a comment based on the corresponding k-grams.\n",
    "    Note that the comment is already tokenized into sentences.\n",
    "    @dis_ngramdic: ngram dictionary as constructed by language_distribution for particular k.\n",
    "    @gramlist: list of kgrams\n",
    "    \"\"\"\n",
    "    score=0\n",
    "    if gramlist:\n",
    "        for gram in gramlist:\n",
    "            score += math.log2(dis_ngramdic.get(gram,1))\n",
    "    return score\n",
    "\n",
    "colnames = None\n",
    "\n",
    "def compute_all_features(lang_dis, original_text, clean_text, structure_text):\n",
    "    \"\"\" This function compares the sentences and structure to each of the languages distributions. It returns\n",
    "    similarity scores to each language model. Also included are other features, such as the number of sentences\n",
    "    per text, etc.\n",
    "    @lang_dis: Language distribution of n-grams.\n",
    "    @clean_text: Text with proper nouns and demonyms substituted\n",
    "    @structure_text: PoS structure retrieved by SENNA.\n",
    "    \"\"\"\n",
    "    simscoredict=dict()\n",
    "    \n",
    "    # For each gramtype, first construct the list of which we can make n-grams.\n",
    "    words_ps = list(word_tokenize(clean_text))\n",
    "    struc_ps = list(word_tokenize(structure_text))\n",
    "    wordlens_ps = [len(word) for word in word_tokenize(original_text) if word.isalpha()]\n",
    "    \n",
    "    # Now we should construct k-gram lists for each k and return the score. Let us store all grams in \n",
    "    for gramtype in lang_dis[list(lang_dis.keys())[0]].keys():\n",
    "        \n",
    "        # Select appropriate data type.\n",
    "        if gramtype == \"tags\":\n",
    "            ps = struc_ps\n",
    "        elif gramtype ==\"words\":\n",
    "            ps = words_ps\n",
    "        elif gramtype == \"w_sizes\":\n",
    "            ps = wordlens_ps\n",
    "        elif gramtype == \"chars\":\n",
    "            ps = clean_text\n",
    "        \n",
    "        # We need to normalize with the sequence length.\n",
    "        seq_len = len(ps)\n",
    "\n",
    "        # For each k, feed the ngrams function into the compute_similarity_score function. \n",
    "        for k in range(1,len(lang_dis[list(lang_dis.keys())[0]][gramtype])+1):\n",
    "            for lang in lang_dis.keys():\n",
    "                simscoredict[lang+'_'+gramtype+'_'+str(k)]= compute_similarity_score(lang_dis[lang][gramtype][k], ngrams(ps,k))/seq_len\n",
    "    \n",
    "    # Set the other features they use in the paper.\n",
    "    simscoredict[\"num_sentences\"] = len(list(sent_tokenize(clean_text)))\n",
    "    simscoredict[\"num_words\"] = len(wordlens_ps)\n",
    "    simscoredict[\"avg_wordlength\"] = sum(wordlens_ps)/len(wordlens_ps)\n",
    "        \n",
    "    global colnames\n",
    "    if colnames == None:\n",
    "        colnames = list(simscoredict.keys())\n",
    "            \n",
    "    return simscoredict.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al Rfou reports having used some different features too. These include:\n",
    "- \"Relative frequency of each of the stop words mentioned in the comment\"\n",
    "- \"Average number of sentences\"\n",
    "- \"Size of the comments\"\n",
    "\n",
    "What these mean is not unequivocally clear. How should relative frequency be measured? Each comment has a deterministic number of sentences, so the average of sentences over what? The size of the comments, measured in what way? Since such features are ambiguous in their definition and are not reported to be important for the native vs non-native experiment, we exclude them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the problems above, the paper does not detail how models were constructed. It mentions that approximately 322K features were used in the experiment and the baseline is 1/(number of classes). The first number makes sense as we have approximately 323K features in total. However, about 110K of these are native US English, whereas the other 200K are non-native speakers. It is not mentioned whether the non-native comments should be downsampled such that we have a balanced problem. We will assume this is the case.\n",
    "\n",
    "### Repetition of the non-native experiment using SVM classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the development/validation set is 7 times as small as the training set. Let us impose the restriction that we only consider n-grams which are at least present 10 times in the entire training set. Computing this distribution takes ~1 hour. Dump it in a pickle object so it can be reloaded later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting parameters\n"
     ]
    }
   ],
   "source": [
    "# Downsample the non-native languages so that the classes are balanced. Fix the random_state such that we get the appropriate lang_dis.\n",
    "print(\"Setting parameters\")\n",
    "n = 4           # n-grams for words, PoS tags and word sizes.\n",
    "m = 4           # m-grams for characters\n",
    "lowerlim = 10   # lower limit on the number of wordcounts to consider words for bigrams, trigrams, etc. Needed to prevent memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deriving the language distribution from training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hans/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:54: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "/home/hans/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:33: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "/home/hans/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "/home/hans/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language distribution constructed in 3678.919601917267 seconds\n"
     ]
    }
   ],
   "source": [
    "# Derive the language distribution from the training data.\n",
    "print(\"Deriving the language distribution from training data...\")\n",
    "start = time.time()\n",
    "lang_dis = pruned_language_distribution(n,m,lowerlim,training[['native','text_clean','text_structure']], training.native.unique())\n",
    "end = time.time()\n",
    "print(\"Language distribution constructed in {} seconds\".format(end-start))\n",
    "\n",
    "# Save it and clear it to save memory.\n",
    "save_object(lang_dis,\"trained_lang_dis\")\n",
    "lang_dis.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the language distribution\n",
      "Computing the features for training and validation data.\n",
      "Finished computing features in 1874.8679361343384 seconds\n"
     ]
    }
   ],
   "source": [
    "# Use the language distribution to obtain features for training data and validation data.\n",
    "print(\"Loading the language distribution\")\n",
    "if 'lang_dis' in globals() or 'lang_dis' in locals():\n",
    "    lang_dis.clear()\n",
    "lang_dis = load_object(\"trained_lang_dis\")\n",
    "\n",
    "print(\"Computing the features for training and validation data.\")\n",
    "start = time.time()\n",
    "features = training.apply(lambda row: compute_all_features(lang_dis,row['text_original'],row['text_clean'], row['text_structure']), axis=1)\n",
    "features = pd.DataFrame(features.to_frame()[0].values.tolist(), index=features.to_frame()[0].index, columns=colnames)\n",
    "training = pd.merge(training, features, left_index=True, right_index=True)\n",
    "features = validation.apply(lambda row: compute_all_features(lang_dis,row['text_original'],row['text_clean'], row['text_structure']), axis=1)\n",
    "features = pd.DataFrame(features.to_frame()[0].values.tolist(), index=features.to_frame()[0].index, columns=colnames)\n",
    "validation = pd.merge(validation, features, left_index=True, right_index=True)\n",
    "end = time.time()\n",
    "print(\"Finished computing features in {} seconds\".format(end-start))\n",
    "\n",
    "# Clean up stuff we no longer need.\n",
    "lang_dis.clear()\n",
    "features = features.iloc[0:0]\n",
    "training = training.drop(['text_original', 'text_clean', 'text_structure'], axis=1)\n",
    "validation = validation.drop(['text_original','text_clean','text_structure'], axis = 1)\n",
    "\n",
    "# Write the training and validation including their features to file.\n",
    "training.to_csv(\"python_data/training_features_4_4_\"+str(lowerlim))\n",
    "validation.to_csv(\"python_data/validation_with_features_4_4_\"+str(lowerlim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data. \n",
    "training=pd.read_csv(\"python_data/training_features_4_4_10\",index_col=0,header=0)\n",
    "validation=pd.read_csv(\"python_data/validation_with_features_4_4_10\",index_col=0,header=0)\n",
    "colnames = training.columns[3:]    #First column native language, second English level, third if native.\n",
    "training.native = training.native.astype('category')\n",
    "validation.native = validation.native.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.726985929712\n"
     ]
    }
   ],
   "source": [
    "# Train the SVC classifier\n",
    "linear = svm.SVC(C=1.0, kernel=\"linear\", penalty=\"l1\", dual=False)\n",
    "linear.fit(training[colnames], training.native)\n",
    "y_predicted = linear.predict(validation[colnames])\n",
    "accur = accuracy_score(validation.native, y_predicted)\n",
    "print(accur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we can achieve a 72.7% accuracy on all the data, which is almost in the league of the 74% mentioned in the paper. Note that accuracy of course depends on the fold on the training and testing data, and exact replication is without reach. Let us investigate if we can bump up this accuracy by parameter tuning through grid search.\n",
    "\n",
    "#### Tuning C.\n",
    "Seems to barely have any effect. Scores hover around .727. In retrospect, this is not entirely unexpected as increasing the parameter C is a means to increase robustness by lowering the bias of the classifier. Since we have lots of training data, we can expect that it will have little effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.72689128651649948,\n",
       " 0.72711212063852604,\n",
       " 0.72720676383368033,\n",
       " 0.72764843207773366,\n",
       " 0.72730140702883461,\n",
       " 0.72761688434601557,\n",
       " 0.72733295476055271,\n",
       " 0.72708057290680805,\n",
       " 0.72752224115086128,\n",
       " 0.7267650955896271,\n",
       " 0.72692283424821758]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = [2**i for i in range(-5, 16, 2)]\n",
    "scores = []\n",
    "colnames = list(training)[3:]\n",
    "for C_ in grid:\n",
    "    linear = svm.LinearSVC(C=C_, penalty=\"l1\", dual=False)\n",
    "    linear.fit(training[colnames], training.native)\n",
    "    y_predicted = linear.predict(validation[colnames])\n",
    "    validation['native'] = np.where(validation['native_lang']=='EN', \"native\", \"non-native\")\n",
    "    scores.append(accuracy_score(validation.native, y_predicted))\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging for non-linear kernels & grid-search to optimize parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines are a generalization of Support Vector Classifiers to non-linear decision boundaries. Such non-linear decision boundaries are constructed by a \"kernel trick\". In Hastie, it is reported that when classes are not linearly separable, non-linear kernels may deliver drastic improvements in prediction performance over linear kernels (but at the risk of overfitting as implied by the higher flexibility).\n",
    "\n",
    "Our data are currently classified into two classes, being (i) native English speakers and (ii) non-native speakers. The second group obviously consists of many subgroups, and the separating hyperplanes between each of these subgroups with the native English speakers can be expected to be different. That the classes are linearly separable is therefore inherently not obvious and finding out whether non-linear kernels do better is an interesting problem.\n",
    "\n",
    "However, problem kicks in as the implementation of `svc` in `sklearn` has in the best-case scenario complexity $O(n\\_features*n\\_samples^2)$, leading to very large processing times for training SVC. Bagging is a neat way around this problem as a reduction in the number of samples by a factor n_estimators yields a reduction in complexity of n_estimators^2, significantly speeding up calculations (inspired/suggested by https://stackoverflow.com/questions/31681373/making-svm-run-faster-in-python). Let us try this by bagging with 20 estimators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier\n",
      "Prediction out-of-sample\n",
      "Bagging SVC trained in 74.2062509059906 seconds reaching accuracy of 72.85317685658401\n"
     ]
    }
   ],
   "source": [
    "# Load data and timing package.\n",
    "import time\n",
    "\n",
    "# Do scaling. This is suggested by Hsu et al. Scaling is based on training data but executed on validation data.\n",
    "scaler = StandardScaler()\n",
    "training[colnames] = scaler.fit_transform(training[colnames])\n",
    "validation[colnames] = scaler.transform(validation[colnames])\n",
    "\n",
    "# Train SVCs by bagging. 20 estimators will yield approximately 10.000 samples per SVM. This number should be feasible\n",
    "# according to the documentation. We can speed up things by multithreading (n_jobs).\n",
    "n_estimators = 20\n",
    "start = time.time()\n",
    "clf = BaggingClassifier(svm.SVC(kernel='rbf', C=2**5, gamma=2**-5, cache_size=2000), random_state = 1281, max_samples=1.0 / n_estimators, n_jobs = -1, n_estimators=n_estimators)\n",
    "print(\"Fitting the classifier\")\n",
    "clf.fit(training[colnames],training.native)\n",
    "print(\"Prediction out-of-sample\")\n",
    "y_predicted = clf.predict(validation[colnames])\n",
    "end = time.time()\n",
    "print(\"Bagging SVC trained in {} seconds reaching accuracy of {}\".format(end - start, 100*accuracy_score(validation.native, y_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With training of the predictor at approximately 90 seconds, grid-search becomes feasible. For this grid search, we follow recommendations by Hsu et al. (\"A practical guide to Support Vector Classification\"). We will refrain from selecting $(C,\\gamma)$ by cross-validation as we have quite a large dataset already. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/99 completed\n",
      "Iteration 10/99 completed\n",
      "Iteration 20/99 completed\n",
      "Iteration 30/99 completed\n",
      "Iteration 40/99 completed\n",
      "Iteration 50/99 completed\n",
      "Iteration 60/99 completed\n",
      "Iteration 70/99 completed\n",
      "Iteration 80/99 completed\n",
      "Iteration 90/99 completed\n",
      "Iteration 99/99 completed\n"
     ]
    }
   ],
   "source": [
    "# Scale the data if this hasn't been done yet.\n",
    "scaler = StandardScaler()\n",
    "training[colnames] = scaler.fit_transform(training[colnames])\n",
    "validation[colnames] = scaler.transform(validation[colnames])\n",
    "\n",
    "# Define the grid and pre-allocate memory for scores.\n",
    "Cs=[2**i for i in range(-5,16,2)]\n",
    "gammas = [2**i for i in range(-15, 3, 2)]\n",
    "param_grid = {'C': Cs, 'gamma': gammas, 'kernel':['rbf'], 'cache_size':[500.0]}\n",
    "scores = pd.DataFrame(columns=['C','gamma','kernel','score'])\n",
    "\n",
    "# Iterate over the grid and save scores for each (C,gamma) pair.\n",
    "itern = 1\n",
    "n_estimators = 20\n",
    "for g in ParameterGrid(param_grid):\n",
    "    svc = svm.SVC()\n",
    "    svc.set_params(**g)\n",
    "    clf = BaggingClassifier(svc, random_state = 1281, max_samples=1.0 / n_estimators, n_jobs = 4, n_estimators=n_estimators)\n",
    "    clf.fit(training[colnames],training.native)\n",
    "    y_predicted = clf.predict(validation[colnames])\n",
    "    g['score'] = [accuracy_score(validation.native, y_predicted)]\n",
    "    holder  = pd.DataFrame(g)\n",
    "    scores = pd.concat([scores, holder])\n",
    "    if (itern==1 or itern%10 == 0 or itern == 99):\n",
    "        print(\"Iteration {}/{} completed\".format(itern, len(Cs)*len(gammas)))\n",
    "    itern += 1\n",
    "        \n",
    "scores.to_csv(\"results_gridsearch_SVM_rbf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspection of the results of the grid search show we reach approximately a 74.4% accuracy if we downsample the validation set such that the dataset is balanced. We balanced the dataset as the training dataset too is balanced. Although not cross-validated, the highest accuracy was found for a gamma of 2^-11 and a C of 8. This results in approximately the same classification performance as in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier\n",
      "Prediction out-of-sample\n",
      "Bagging SVC score: 0.746166950596\n"
     ]
    }
   ],
   "source": [
    "# Train SVCs by bagging. 20 estimators will yield approximately 10.000 samples per SVM. This number should be feasible\n",
    "# according to the documentation. We can speed up things by multithreading (n_jobs).\n",
    "n_estimators = 20\n",
    "clf = BaggingClassifier(svm.SVC(kernel='rbf', C=2**3, gamma=2**-11, cache_size=2000), random_state = 1281, max_samples=1.0 / n_estimators, n_jobs = 4, n_estimators=n_estimators)\n",
    "print(\"Fitting the classifier\")\n",
    "clf.fit(training[colnames],training.native)\n",
    "print(\"Prediction out-of-sample\")\n",
    "y_predicted = clf.predict(validation[colnames])\n",
    "print(\"Bagging SVC score:\",accuracy_score(validation.native, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = pd.DataFrame(y_predicted, validation.index)\n",
    "y_predicted.columns = [\"prediction\"]\n",
    "y_predicted = y_predicted.join(validation[[\"native_lang\",\"level_english\",\"native\",\"num_words\"]])\n",
    "y_predicted.to_csv(\"output_SVM_RBF_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing bagging with LinearSVC shows that the increase in performance is likely due to the non-linear kernel, not bagging itself. Let us try it to by decreasing the number of samples used for training. This might be a good idea to reduce the bias in the unflexible LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier\n",
      "Prediction out-of-sample\n",
      "Bagging LinearSVC score for max_sample = 0.000244140625:     0.7208025742949082\n",
      "Fitting the classifier\n",
      "Prediction out-of-sample\n",
      "Bagging LinearSVC score for max_sample = 0.0009765625:     0.7208025742949082\n",
      "Fitting the classifier\n",
      "Prediction out-of-sample\n",
      "Bagging LinearSVC score for max_sample = 0.00390625:     0.7208025742949082\n",
      "Fitting the classifier\n",
      "Prediction out-of-sample\n",
      "Bagging LinearSVC score for max_sample = 0.015625:     0.7208025742949082\n"
     ]
    }
   ],
   "source": [
    "n_estimators = 20\n",
    "max_samples=[2**i for i in range(-12,-4,2)]\n",
    "scores = []\n",
    "for max_sample in max_samples:\n",
    "    clf = BaggingClassifier(svm.LinearSVC(C=2**3), random_state = 1281, max_samples=max_sample, n_jobs = 5, n_estimators=n_estimators)\n",
    "    print(\"Fitting the classifier\")\n",
    "    clf.fit(training[colnames],training.native)\n",
    "    print(\"Prediction out-of-sample\")\n",
    "    y_predicted = clf.predict(validation[colnames])\n",
    "    scores.append(accuracy_score(validation.native, y_predicted))\n",
    "    print(\"Bagging LinearSVC score for max_sample = {}:     {}\".format(max_sample,accuracy_score(validation.native, y_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "We also want to try Random Forests for classification. An initial implementation reaches an accuracy of 72.4%, which is quite high already. Let us see if we can improve it by cross-validation. The cross-validation implementation was taken from https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb. Note that training a random forest is substantially faster than the SVM classifier, so we can allow ourselves to try more parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.730456180201\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=300, min_samples_leaf = 20, n_jobs = 4)\n",
    "rf.fit(training[colnames],training.native)\n",
    "y_predicted = rf.predict(validation[colnames])\n",
    "print(accuracy_score(y_predicted,validation.native))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed: 64.7min\n",
      "[Parallel(n_jobs=6)]: Done 150 out of 150 | elapsed: 249.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 76,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 10,\n",
       " 'min_samples_split': 20,\n",
       " 'n_estimators': 250}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 700, num = 11)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 10)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [20, 50, 100]\n",
    "min_samples_leaf = [10, 20, 50, 100]\n",
    "\n",
    "# Create the grid.\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "# Allocate dataframe for the resulting scores.\n",
    "scores_rf = pd.DataFrame(columns=['n_estimators','max_features','max_depth','min_samples_split','min_samples_leaf','bootstrap'])\n",
    "\n",
    "# Start training and calculate accuracy for each set.\n",
    "rf = RandomForestClassifier(random_state = 42)\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n",
    "                              n_iter = 50, cv = 3, verbose=1, random_state=42, n_jobs=6)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(training[colnames], training.native);\n",
    "\n",
    "# Return the best parameter set.\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73118177803\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=250, min_samples_leaf = 10, min_samples_split = 20, max_depth = 76, max_features = 'sqrt', n_jobs = 4)\n",
    "rf.fit(training[colnames],training.native)\n",
    "y_predicted = rf.predict(validation[colnames])\n",
    "print(accuracy_score(y_predicted,validation.native))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open problems\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### Benchmark language_distribution against pruned_language_distribution\n",
    "\n",
    "Here we benchmark the function `pruned_language_distribution` against `language_distribution` for a small sample of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def language_distribution(n, m, training, LANGUAGES):\n",
    "    \"\"\"Calculate the word n grams distribution up to n, the character n gram distribution up to m.\n",
    "    @n: consider k-grams up to and including n for words, part of speech tags and word sizes.\n",
    "    @m: consider k-grams up to and including m for characters.\n",
    "    @training: training data to retrieve the language distribution from.\n",
    "    @LANGUAGES: languages based on which we classify.\n",
    "    \"\"\"\n",
    "    \n",
    "    language_dist = {}\n",
    "\n",
    "    for language in LANGUAGES:\n",
    "        language_dist[language] = {\"words\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)])),\n",
    "                               \"tags\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)])),\n",
    "                               \"chars\": dict(zip(range(1, m+1), [FreqDist() for i in range(1, m+1)])),\n",
    "                               \"w_sizes\": dict(zip(range(1, n+1), [FreqDist() for i in range(1, n+1)]))}\n",
    "    \n",
    "    for language, tokenized_sents, tokenized_struc in training.itertuples(index=False):\n",
    "    \n",
    "        # Construct n grams counts from the tokenized sentences.\n",
    "        for sentence in tokenized_sents:\n",
    "            token = word_tokenize(sentence)     \n",
    "            wordlens = [len(word) for word in token if word.isalpha()]   \n",
    "            \n",
    "            for k in range(1,n+1):  \n",
    "                language_dist[language][\"w_sizes\"][k].update(ngrams(wordlens,k))\n",
    "                language_dist[language][\"words\"][k].update(ngrams(token,k))\n",
    "                \n",
    "        # Construct n gram counts from sentences tokenized based on structure.\n",
    "        for sentence in tokenized_struc:\n",
    "            token = word_tokenize(sentence)\n",
    "            for k in range(1,n+1):\n",
    "                language_dist[language][\"tags\"][k].update(ngrams(token,k))\n",
    "\n",
    "        # Construct character m-grams for tokenized sentences.\n",
    "        for sentence in tokenized_sents:\n",
    "            for k in range(1,m+1):\n",
    "                language_dist[language][\"chars\"][k].update(ngrams(sentence,k))\n",
    "    \n",
    "    return language_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "def sum_keys(d):\n",
    "    return (0 if not isinstance(d, dict) else len(d) + sum(sum_keys(v) for v in d.values()))\n",
    "\n",
    "rand_sample = df.sample(20000)\n",
    "rand_sample['tokenized_sents'] = rand_sample.apply(lambda row: sent_tokenize(row['text_clean']), axis=1)\n",
    "rand_sample['tokenized_struc'] = rand_sample.apply(lambda row: sent_tokenize(row['text_structure']), axis=1)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "dis1 = language_distribution(4, 9, rand_sample[['native','tokenized_sents','tokenized_struc']], rand_sample.native.unique())\n",
    "end = timeit.default_timer()\n",
    "\n",
    "dis2 = pruned_language_distribution(4, 9, 1, rand_sample[['native','text_clean','text_structure']], rand_sample.native.unique())\n",
    "end2 = timeit.default_timer()\n",
    "\n",
    "print(\"Unpruned, time: {} sec, size: {} items, \\n Pruned, time: {} sec, size: {} items\".format(end-start, sum_keys(dis1), end2-end, sum_keys(dis2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is evident the pruned models are already somewhat better in terms of memory and take only twice as long for construction. If we increase `lowerlimitfreq` this obviously becomes much better. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
